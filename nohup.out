[32m[I 2022-10-23 05:31:44,667][0m A new study created in memory with name: no-name-c8694c0a-3f3c-45b8-82e9-e92e82f69c94[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Study name: no-name-c8694c0a-3f3c-45b8-82e9-e92e82f69c94[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Directions: ['maximize'][0m
[[36m2022-10-23 05:31:44,670[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 05:31:44,670[0m][[35mHYDRA[0m] 	#0 : trainer.max_epochs=300 model.optimizer.lr=0.0019960425587510337 model.eta=12.820066649756805 model.gamma=0.493954965106403 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 05:31:45,547[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 05:31:45,550[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0019960425587510337                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 12.820066649756805                                                 
│       gamma: 0.493954965106403                                                
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 05:31:45,584[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 05:31:45,585[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 05:31:45,785[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
[[36m2022-10-23 05:31:45,895[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp0574k1u4[0m
[[36m2022-10-23 05:31:45,895[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp0574k1u4/_remote_module_non_scriptable.py[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 05:31:45,913[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 05:31:45,913[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 05:31:45,915[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: flpie. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_053147-3lq2l36n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-frost-66
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3lq2l36n
[[36m2022-10-23 05:31:51,433[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 05:31:51,436[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 05:31:51,437[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 05:31:51,438[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 05:31:51,444[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 86%|████████▌ | 6417/7466 [00:00<00:00, 64164.71it/s]100%|██████████| 7466/7466 [00:00<00:00, 64260.27it/s]
[[36m2022-10-23 05:31:52,231[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_8c08e2 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_8c0968 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('train/accu', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('shd', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/acc')` could not find the monitored key in the returned metrics: ['train/loss', 'train/losses', 'train/accu', 'shd', 'tpr', 'fdr', 'epoch', 'step']. HINT: Did you call `log('val/acc', value)` in the `LightningModule`?
  warning_cache.warn(m)
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 224.21it/s loss: 1.23e+03   
                                                               v_num: l36n      
[[36m2022-10-23 05:42:18,151[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 05:42:18,151[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 56.288 MB of 56.288 MB uploaded (0.000 MB deduped)wandb: \ 56.288 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: | 56.288 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: / 56.288 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: - 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: \ 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: | 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: / 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: - 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: \ 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: | 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: / 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▄▂▆▄▁▆▃█▅▂▇▅▂▇▄▁▆▃█▅▃▇▅▂▇▄▂▆▄▁▆▃█
wandb:                 fdr █▇▇▇█▇▇▇▇▇▇▇▇▇▆▇▇▇▇█▇██▆███████▅█▁▁▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd █▇▆▆██▇▇██▇▇▆▅▃▄▅▄▃▃▂▂▃▁▂▃▃▃▂▂▂▁▂▁▁▁▁▁▁▁
wandb:                 tpr ▂▅██▄▅▇▄▇▅▅▄▄▅▇▂▄▂▂▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▁▂▁▁
wandb:          train/loss ▃▁▁▁▁▁▁▃▁▄▁▁▁█▁▁▁▁▂▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.002
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1659.35413
wandb: trainer/global_step 104999
wandb: 
wandb: Synced dulcet-frost-66: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3lq2l36n
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_053147-3lq2l36n/logs
[[36m2022-10-23 05:42:22,256[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/0[0m
[[36m2022-10-23 05:42:22,256[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 05:42:22,259[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 05:42:22,259[0m][[35mHYDRA[0m] 	#1 : trainer.max_epochs=300 model.optimizer.lr=0.002798666792298152 model.eta=6.252820847718837 model.gamma=0.8216849597815173 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 05:42:22,374[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 05:42:22,375[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.002798666792298152                                              
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 6.252820847718837                                                  
│       gamma: 0.8216849597815173                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 05:42:22,402[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 05:42:22,402[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 05:42:22,403[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 05:42:22,419[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 05:42:22,419[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 05:42:22,422[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_054222-1d20456c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-firebrand-67
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/1d20456c
[[36m2022-10-23 05:42:27,008[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 05:42:27,016[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 05:42:27,016[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 05:42:27,022[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 77045.86it/s]
[[36m2022-10-23 05:42:27,127[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_076e91 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_076f0b │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 227.10it/s loss: 4.19e+03   
                                                               v_num: 456c      
[[36m2022-10-23 05:52:43,189[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 05:52:43,189[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 58.389 MB of 58.389 MB uploaded (0.000 MB deduped)wandb: \ 58.389 MB of 58.389 MB uploaded (0.000 MB deduped)wandb: | 58.389 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: / 58.389 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: - 58.389 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: \ 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: | 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: / 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: - 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: \ 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: | 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: / 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: - 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▃▇▅▂▇▄▁▅▂▇▄▁▆▂▇▄▁▆▃█▄▁▆▃█▅▁▆▃█▅▂▇
wandb:                 fdr █▇▇▇▇▇▇▇▇▇▆▆▇▆▆▇▇▆▅▆▆▇▆▅▇▆▁▁█▁███▁▁▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▅▅▇▆▆█▆▇▆▆▄▅▅▃▂▃▃▂▁▁▁▂▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁
wandb:                 tpr ▂▃▆▅▆▅▅▅█▇█▆▄▆▄▄▃▄▅▃▂▂▂▃▂▂▁▁▁▁▁▁▁▁▂▂▁▂▁▁
wandb:          train/loss ▂▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▂▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.0028
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 3325.89307
wandb: trainer/global_step 100799
wandb: 
wandb: Synced trim-firebrand-67: https://wandb.ai/flpie/DAG_GCN_optuna/runs/1d20456c
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_054222-1d20456c/logs
[[36m2022-10-23 05:52:47,449[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/1[0m
[[36m2022-10-23 05:52:47,449[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 05:52:47,452[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 05:52:47,452[0m][[35mHYDRA[0m] 	#2 : trainer.max_epochs=300 model.optimizer.lr=0.0036423909725828802 model.eta=10.518907384945715 model.gamma=0.7151166416549226 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 05:52:47,563[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 05:52:47,564[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0036423909725828802                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10.518907384945715                                                 
│       gamma: 0.7151166416549226                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 05:52:47,591[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 05:52:47,591[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 05:52:47,592[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 05:52:47,608[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 05:52:47,608[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 05:52:47,610[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 05:52:47,610[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 05:52:47,610[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 05:52:47,611[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 05:52:47,611[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_055247-3ahkpve2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-pine-68
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahkpve2
[[36m2022-10-23 05:52:52,192[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 05:52:52,199[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 05:52:52,200[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 05:52:52,201[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 05:52:52,207[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 76%|███████▌  | 5662/7466 [00:00<00:00, 30230.55it/s]100%|██████████| 7466/7466 [00:00<00:00, 35314.22it/s]
[[36m2022-10-23 05:52:52,425[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_7c12d4 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_7c134f │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 218.52it/s loss: 874 v_num: 
                                                               pve2             
[[36m2022-10-23 06:03:34,886[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:03:34,886[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 62.316 MB of 62.316 MB uploaded (0.000 MB deduped)wandb: \ 62.316 MB of 62.316 MB uploaded (0.000 MB deduped)wandb: | 62.316 MB of 62.316 MB uploaded (0.000 MB deduped)wandb: / 62.316 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: - 62.316 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: \ 62.316 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: | 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: / 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: - 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: \ 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: | 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: / 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: - 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: \ 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▄▂▆▄▁▆▃█▅▂▇▅▂▇▄▁▆▃█▅▃▇▅▂▇▄▂▆▄▁▆▃█
wandb:                 fdr ▇▇▇▇▇▇▇▆▇▆▇▇▇▇█▆▅▇▇████▆▂▆▁▅█▅▆███▁▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▅▅▇█▇▆▇▅▆▆▅▅▇▅▄▃▂▂▃▃▃▂▄▂▁▂▁▁▂▁▂▂▂▂▂▂▂▂▂▂
wandb:                 tpr ▂▄▆█▇▇▇▇▇█▄▃▃▃▂▄▅▂▂▁▁▁▂▂▄▂▂▂▁▃▂▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▁▁▃▁▁▂▂▂▂█▂▁▃▇▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00364
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1160.40161
wandb: trainer/global_step 104999
wandb: 
wandb: Synced spring-pine-68: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahkpve2
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_055247-3ahkpve2/logs
[[36m2022-10-23 06:03:39,795[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/2[0m
[[36m2022-10-23 06:03:39,795[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:03:39,798[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:03:39,798[0m][[35mHYDRA[0m] 	#3 : trainer.max_epochs=300 model.optimizer.lr=0.005655842242049688 model.eta=10.558580140848385 model.gamma=0.11239160463161402 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 06:03:39,908[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:03:39,909[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.GELU                                               
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.005655842242049688                                              
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10.558580140848385                                                 
│       gamma: 0.11239160463161402                                              
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 06:03:39,935[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:03:39,935[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:03:39,936[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:03:39,951[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:03:39,952[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:03:39,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:03:39,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:03:39,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:03:39,954[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:03:39,954[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_060339-3ahearvr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-armadillo-69
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahearvr
[[36m2022-10-23 06:03:44,561[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:03:44,564[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:03:44,564[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:03:44,564[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:03:44,565[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:03:44,565[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:03:44,565[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:03:44,566[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:03:44,572[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 75222.13it/s]
[[36m2022-10-23 06:03:44,678[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_00e695 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ GELU              │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ GELU              │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_00e70a │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ GELU              │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ GELU              │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 225.01it/s loss: 1.96e+03   
                                                               v_num: arvr      
[[36m2022-10-23 06:14:44,430[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:14:44,431[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 75.561 MB of 75.561 MB uploaded (0.000 MB deduped)wandb: \ 75.561 MB of 75.561 MB uploaded (0.000 MB deduped)wandb: | 75.561 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: / 75.561 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: - 75.561 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: \ 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: | 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: / 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: - 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: \ 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: | 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: / 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▄▂▆▄▁▆▃█▅▂▇▅▂▇▄▁▆▃█▅▃▇▅▂▇▄▂▆▄▁▆▃█
wandb:                 fdr ▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▆▆▆▇▇▇▅▇▇▆▃▅▆▆▅█▇████▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▆███▆▇██▇▇█▇▆█▆▆▃▃▄▂▃▂▃▂▃▁▁▁▂▂▂▂▂▁▂▂▁▁▁▁
wandb:                 tpr ▄█▇▇▇▇▆▆▅▅▆▅▄▅▅▆▄▅▂▂▂▄▃▁▃▃▄▁▃▃▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▂▁▄▂▂▁▁▂▁█▃▂▂▆▁▂▂▂▃▂▁▂▁▁▂▁▂▁▁▃▁▁▁▁▁▁▄▂▁▂
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00566
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 2564.73486
wandb: trainer/global_step 104999
wandb: 
wandb: Synced solar-armadillo-69: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahearvr
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_060339-3ahearvr/logs
[[36m2022-10-23 06:14:49,103[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/3[0m
[[36m2022-10-23 06:14:49,103[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:14:49,106[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:14:49,106[0m][[35mHYDRA[0m] 	#4 : trainer.max_epochs=300 model.optimizer.lr=0.0037123712406235856 model.eta=12.69252739023638 model.gamma=0.16784311747867892 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 06:14:49,217[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:14:49,218[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.GELU                                               
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0037123712406235856                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 12.69252739023638                                                  
│       gamma: 0.16784311747867892                                              
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 06:14:49,246[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:14:49,246[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:14:49,247[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:14:49,263[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:14:49,264[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:14:49,265[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:14:49,265[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:14:49,265[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:14:49,266[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:14:49,266[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_061449-zeseqxfj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-forest-70
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/zeseqxfj
[[36m2022-10-23 06:14:53,858[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:14:53,861[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:14:53,862[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:14:53,868[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 74914.77it/s]
[[36m2022-10-23 06:14:53,977[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_8fd75f │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ GELU              │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ GELU              │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_8fd7d9 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ GELU              │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ GELU              │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 224.13it/s loss: 692 v_num: 
                                                               qxfj             
[[36m2022-10-23 06:26:13,242[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:26:13,243[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 72.806 MB of 72.806 MB uploaded (0.000 MB deduped)wandb: \ 72.806 MB of 72.806 MB uploaded (0.000 MB deduped)wandb: | 72.806 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: / 72.806 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: - 72.821 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: \ 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: | 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: / 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: - 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: \ 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: | 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: / 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▇▄▂▇▄▁▆▃█▅▃▇▆▄▁▆▃█▅▂▇▅▂▇▄▃█▅▂▇▄▂▆▄▁▆▃█▇
wandb:                 fdr ▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▇▅▆▅▅▅▇▆▇▆▅▃▄▄▆▇▁▁▁▁██▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▃▅▅██▇█▇▆▆▅▆▅▅▃▃▃▄▁▂▂▁▁▃▂▄▂▁▁▁▁▂▂▁▁▁▁▂▂▁
wandb:                 tpr ▅▅██▇▇▇█▆▅▆▄▅▇▇▅▆▄▅▄▄▅▅▃▄▃▂▃▂▅▅▂▂▁▂▂▁▁▁▁
wandb:          train/loss ▃▁▁▁▁▃▄▁▂▁▂▂▃▂▃▁▃▂▁▄▂▂▂▆▂▃▂▂▄▁▂▁▁▁▁█▂▁▂▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00371
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 720.3266
wandb: trainer/global_step 109199
wandb: 
wandb: Synced dark-forest-70: https://wandb.ai/flpie/DAG_GCN_optuna/runs/zeseqxfj
wandb: Synced 6 W&B file(s), 1612 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_061449-zeseqxfj/logs
[[36m2022-10-23 06:26:18,718[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/4[0m
[[36m2022-10-23 06:26:18,718[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:26:18,724[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:26:18,724[0m][[35mHYDRA[0m] 	#5 : trainer.max_epochs=300 model.optimizer.lr=0.0003927308476779979 model.eta=18.880866123061907 model.gamma=0.4070138486494157 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 06:26:18,834[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:26:18,836[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0003927308476779979                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 18.880866123061907                                                 
│       gamma: 0.4070138486494157                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 06:26:18,863[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:26:18,863[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:26:18,864[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:26:18,880[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:26:18,880[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:26:18,883[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_062618-3p8l4492
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-wind-71
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3p8l4492
[[36m2022-10-23 06:26:23,432[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:26:23,435[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:26:23,439[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:26:23,440[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:26:23,446[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 76%|███████▌  | 5661/7466 [00:00<00:00, 28508.90it/s]100%|██████████| 7466/7466 [00:00<00:00, 33295.52it/s]
[[36m2022-10-23 06:26:23,678[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_2ae298 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_2ae311 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 223.64it/s loss: 181 v_num: 
                                                               4492             
[[36m2022-10-23 06:36:02,908[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:36:02,908[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 42.631 MB of 42.631 MB uploaded (0.000 MB deduped)wandb: \ 42.631 MB of 42.631 MB uploaded (0.000 MB deduped)wandb: | 42.631 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: / 42.631 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: - 42.631 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: \ 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: | 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: / 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: - 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: \ 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: | 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: / 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: - 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃█▃█▅▃▆▃█▄▁▆▃▆▄▁▄▁▆▄▇▄▁▅▂▇▄▇▄▂▅▂▇▄▇▅▂▇
wandb:                 fdr █████▇▆▆▆▇▇▇▇████████▁▁▁▁███▁▁██████████
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▆█▆▆▇▆▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▂▂▂▂▄▂▂▃▄▄
wandb:                 tpr ▁▁▁▁▁▃▆▆█▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▆▄█▃▄█▁▂▄▁▂▂▂▃▆▁▂▃▂▄▁▁▁▃▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 1.0
wandb:             lr-Adam 0.00039
wandb:                 shd 22.0
wandb:                 tpr 0.0
wandb:          train/loss 157.81535
wandb: trainer/global_step 96599
wandb: 
wandb: Synced charmed-wind-71: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3p8l4492
wandb: Synced 6 W&B file(s), 1426 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_062618-3p8l4492/logs
[[36m2022-10-23 06:36:08,109[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/5[0m
[[36m2022-10-23 06:36:08,110[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:36:08,114[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:36:08,114[0m][[35mHYDRA[0m] 	#6 : trainer.max_epochs=300 model.optimizer.lr=0.009175498627796746 model.eta=1.6364139758502319 model.gamma=0.46781544701426847 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 06:36:08,223[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:36:08,224[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.009175498627796746                                              
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 1.6364139758502319                                                 
│       gamma: 0.46781544701426847                                              
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 06:36:08,249[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:36:08,249[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:36:08,251[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:36:08,265[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:36:08,266[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:36:08,267[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:36:08,267[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:36:08,267[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:36:08,268[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:36:08,268[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_063608-leq0nids
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-plant-72
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/leq0nids
[[36m2022-10-23 06:36:13,596[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:36:13,599[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:36:13,600[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:36:13,601[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:36:13,607[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 98%|█████████▊| 7296/7466 [00:00<00:00, 72953.46it/s]100%|██████████| 7466/7466 [00:00<00:00, 72768.47it/s]
[[36m2022-10-23 06:36:13,716[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_8a2fc3 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_8a3038 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 229.57it/s loss: 1.38e+05   
                                                               v_num: nids      
[[36m2022-10-23 06:48:17,407[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:48:17,407[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 90.254 MB of 90.254 MB uploaded (0.000 MB deduped)wandb: \ 90.254 MB of 90.254 MB uploaded (0.000 MB deduped)wandb: | 90.254 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: / 90.254 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: - 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: \ 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: | 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: / 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: - 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: \ 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: | 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: / 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▇▅▂▇▅▃█▅▂▁▆▃█▅▄▂▆▄▁▇▅▂▇▄▃█▅▂▇▆▃█▅▃▂▆▄▁▇
wandb:                 fdr ▇▇▆▇▇▇▆▇▇▇▆▆▆▆▆▆▆▇▇▇▇▆▆▇▅█▇▇▅▆▇▇██▆▆▆▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▆▆▅██▆▅▇▆▆▄▄▅▅▄▅▅▄▄▄▅▂▂▂▁▃▂▂▁▁▂▂▂▂▁▁▁▁▁▁
wandb:                 tpr ▆▇█▆▆█▆█▆▄▇▇▇▆▆▆▆▃▃▂▂▃▃▂▃▁▁▁▃▃▁▂▁▁▁▁▁▁▁▁
wandb:          train/loss ▁▁▁▁▁▁▁▁▁▃▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▁▄▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00918
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 165410.32812
wandb: trainer/global_step 113399
wandb: 
wandb: Synced wise-plant-72: https://wandb.ai/flpie/DAG_GCN_optuna/runs/leq0nids
wandb: Synced 6 W&B file(s), 1674 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_063608-leq0nids/logs
[[36m2022-10-23 06:48:22,423[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/6[0m
[[36m2022-10-23 06:48:22,424[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:48:22,428[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:48:22,428[0m][[35mHYDRA[0m] 	#7 : trainer.max_epochs=300 model.optimizer.lr=0.0003902135171882364 model.eta=17.337087295156007 model.gamma=0.9683057858638486 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 06:48:22,542[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:48:22,543[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0003902135171882364                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 17.337087295156007                                                 
│       gamma: 0.9683057858638486                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 06:48:22,570[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:48:22,570[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:48:22,571[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:48:22,587[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:48:22,587[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:48:22,588[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_064822-rnx8ttjd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-eon-73
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/rnx8ttjd
[[36m2022-10-23 06:48:26,321[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:48:26,325[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:48:26,325[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:48:26,326[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:48:26,331[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 76517.22it/s]
[[36m2022-10-23 06:48:26,436[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_3fe041 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_3fe0b9 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 215.69it/s loss: 984 v_num: 
                                                               ttjd             
[[36m2022-10-23 06:58:09,741[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:58:09,741[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 42.280 MB of 42.280 MB uploaded (0.000 MB deduped)wandb: \ 42.280 MB of 42.280 MB uploaded (0.000 MB deduped)wandb: | 42.280 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: / 42.280 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: - 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: \ 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: | 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: / 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: - 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: \ 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: | 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: / 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃█▃█▅▃▆▃█▄▁▆▃▆▄▁▄▁▆▄▇▄▁▅▂▇▄▇▄▂▅▂▇▄▇▅▂▇
wandb:                 fdr ███▇▇▇▇▇▇▆▇▇██████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▆█▇▅▄▄▄▅▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 tpr ▁▁▁▅██████▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▃▂▃▂█▃▁▂▂▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00039
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1030.89514
wandb: trainer/global_step 96599
wandb: 
wandb: Synced wise-eon-73: https://wandb.ai/flpie/DAG_GCN_optuna/runs/rnx8ttjd
wandb: Synced 6 W&B file(s), 1426 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_064822-rnx8ttjd/logs
[[36m2022-10-23 06:58:15,140[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/7[0m
[[36m2022-10-23 06:58:15,140[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:58:15,145[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:58:15,145[0m][[35mHYDRA[0m] 	#8 : trainer.max_epochs=300 model.optimizer.lr=0.006727248676069746 model.eta=14.9122122598956 model.gamma=0.5981938024256199 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 06:58:15,260[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:58:15,261[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.GELU                                               
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.006727248676069746                                              
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 14.9122122598956                                                   
│       gamma: 0.5981938024256199                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 06:58:15,287[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:58:15,288[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:58:15,289[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:58:15,304[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:58:15,304[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:58:15,306[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:58:15,306[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:58:15,306[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:58:15,307[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:58:15,307[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_065815-aoa2vckp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-pond-74
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/aoa2vckp
[[36m2022-10-23 06:58:19,817[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:58:19,820[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:58:19,821[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:58:19,822[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:58:19,831[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 99%|█████████▉| 7418/7466 [00:00<00:00, 74173.53it/s]100%|██████████| 7466/7466 [00:00<00:00, 73999.17it/s]
[[36m2022-10-23 06:58:19,939[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_a129bc │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ GELU              │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ GELU              │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_a12a34 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ GELU              │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ GELU              │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 228.95it/s loss: 3.1e+04    
                                                               v_num: vckp      
[[36m2022-10-23 07:09:04,876[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:09:04,876[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 75.198 MB of 75.198 MB uploaded (0.000 MB deduped)wandb: \ 75.198 MB of 75.198 MB uploaded (0.000 MB deduped)wandb: | 75.198 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: / 75.198 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: - 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: \ 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: | 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: / 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: - 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: \ 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: | 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: / 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▃▇▅▂▇▄▁▅▂▇▄▁▆▂▇▄▁▆▃█▄▁▆▃█▅▁▆▃█▅▂▇
wandb:                 fdr ▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▅▆▅▆▆██▆▁▁▁█
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▇▆▇▇▇█▅▆█▇▆▆▆▄▅▄▄▇▇▇▇▇▆▅▃▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁
wandb:                 tpr ▇▆▇▇█▅█▆▅▄▅▆▄▃▃▃▃▆▄▄▄▄▃▃▃▄▂▂▂▃▂▂▂▁▁▂▂▁▁▁
wandb:          train/loss ▂▁▃▃▂▂▁▂▆▁▇▁▁▁▂▃▂▇█▂▃▁▂▁▂▂▂▂▁▂▂▂▄▄▃▁▁▅▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 1.0
wandb:             lr-Adam 0.00673
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 39855.86719
wandb: trainer/global_step 100799
wandb: 
wandb: Synced distinctive-pond-74: https://wandb.ai/flpie/DAG_GCN_optuna/runs/aoa2vckp
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_065815-aoa2vckp/logs
[[36m2022-10-23 07:09:09,448[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/8[0m
[[36m2022-10-23 07:09:09,448[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:09:09,453[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:09:09,453[0m][[35mHYDRA[0m] 	#9 : trainer.max_epochs=300 model.optimizer.lr=0.0019229650606389275 model.eta=6.941287542379018 model.gamma=0.3269184093556379 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:09:09,567[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:09:09,568[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0019229650606389275                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 6.941287542379018                                                  
│       gamma: 0.3269184093556379                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 07:09:09,595[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:09:09,595[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:09:09,596[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:09:09,612[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:09:09,612[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:09:09,613[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:09:09,613[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:09:09,614[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:09:09,614[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:09:09,614[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_070909-13cmg0ep
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sound-75
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/13cmg0ep
[[36m2022-10-23 07:09:14,648[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:09:14,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:09:14,652[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:09:14,653[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:09:14,659[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 99%|█████████▊| 7362/7466 [00:00<00:00, 73610.94it/s]100%|██████████| 7466/7466 [00:00<00:00, 73399.73it/s]
[[36m2022-10-23 07:09:14,767[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_27292c │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_2729a6 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 221.47it/s loss: 1.11e+03   
                                                               v_num: g0ep      
[[36m2022-10-23 07:19:34,397[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:19:34,397[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 50.714 MB of 50.714 MB uploaded (0.000 MB deduped)wandb: \ 50.714 MB of 50.714 MB uploaded (0.000 MB deduped)wandb: | 50.714 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: / 50.714 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: - 50.734 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: \ 50.734 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: | 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: / 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: - 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: \ 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: | 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: / 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: - 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▃▇▅▂▇▄▁▅▂▇▄▁▆▂▇▄▁▆▃█▄▁▆▃█▅▁▆▃█▅▂▇
wandb:                 fdr █▇▇▇▇▇▇▇▇▇▇▇▇██▇████▇▇███████▁███▁▁▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▆▆▅▆▇█▅▇▇▅▆▅▅▅▅▅▇▅▃▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 tpr ▃▅▆▆▆▆█▅█▅▅▅▅▁▁▅▁▁▁▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▂▁▁▁▁▁▁▁▃▁█▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00192
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1021.60461
wandb: trainer/global_step 100799
wandb: 
wandb: Synced royal-sound-75: https://wandb.ai/flpie/DAG_GCN_optuna/runs/13cmg0ep
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_070909-13cmg0ep/logs
[[36m2022-10-23 07:19:38,794[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/9[0m
[[36m2022-10-23 07:19:38,794[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:19:38,798[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:19:38,799[0m][[35mHYDRA[0m] 	#10 : trainer.max_epochs=300 model.optimizer.lr=0.007514548626225465 model.eta=6.4873111276534186 model.gamma=0.6207304740806995 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 07:19:38,911[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:19:38,912[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.GELU                                               
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.GELU                                             
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.007514548626225465                                              
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 6.4873111276534186                                                 
│       gamma: 0.6207304740806995                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 07:19:38,939[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:19:38,939[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:19:38,940[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:19:38,956[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:19:38,956[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:19:38,957[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_071938-2luc429r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-thunder-76
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2luc429r
[[36m2022-10-23 07:19:43,684[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:19:43,688[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:19:43,689[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:19:43,695[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|█████████▉| 7457/7466 [00:00<00:00, 74563.85it/s]100%|██████████| 7466/7466 [00:00<00:00, 74359.57it/s]
[[36m2022-10-23 07:19:43,802[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_9e476b │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ GELU              │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ GELU              │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_9e47e7 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ GELU              │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ GELU              │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 231.84it/s loss: 6.75e+04   
                                                               v_num: 429r      
[[36m2022-10-23 07:30:56,037[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:30:56,037[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 78.856 MB of 78.856 MB uploaded (0.000 MB deduped)wandb: \ 78.856 MB of 78.856 MB uploaded (0.000 MB deduped)wandb: | 78.856 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: / 78.856 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: - 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: \ 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: | 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: / 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: - 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: \ 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: | 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: / 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▄▂▆▄▁▆▃█▅▂▇▅▂▇▄▁▆▃█▅▃▇▅▂▇▄▂▆▄▁▆▃█
wandb:                 fdr ▇▇▇▆▇▇▇▇▇▇▆▇▇▆▆▅▇▆▆▇█▇▆▇▇▇▇▅▆▅▅▁▁▁▅▅▆███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▅▆█▅█▇▇█▇█▄▆█▆▄▂▅▅▄▅▆▄▃▃▂▃▂▁▂▁▁▁▁▁▁▁▁▂▂▂
wandb:                 tpr ▅▆▇▇▆▆▆█▆▇▆▅▅▆▅▅▃▅▅▂▁▂▃▁▁▂▁▄▂▂▂▂▂▁▂▂▂▁▁▁
wandb:          train/loss ▂▁▅▂▁▁▁▂▁█▃▂▂▆▁▁▂▂▃▂▁▂▁▂▂▁▁▂▂▃▁▁▁▁▃▂▄▂▂▂
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 1.0
wandb:             lr-Adam 0.00751
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 90967.5625
wandb: trainer/global_step 104999
wandb: 
wandb: Synced devoted-thunder-76: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2luc429r
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_071938-2luc429r/logs
[[36m2022-10-23 07:31:01,255[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/10[0m
[[36m2022-10-23 07:31:01,255[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:31:01,260[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:31:01,260[0m][[35mHYDRA[0m] 	#11 : trainer.max_epochs=300 model.optimizer.lr=0.002796126334085001 model.eta=4.909994085152537 model.gamma=0.8365797926796967 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:31:01,372[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:31:01,373[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.002796126334085001                                              
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 4.909994085152537                                                  
│       gamma: 0.8365797926796967                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 07:31:01,400[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:31:01,400[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:31:01,401[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:31:01,529[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:31:01,529[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:31:01,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:31:01,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:31:01,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:31:01,532[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:31:01,532[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_073101-2shqld3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-spaceship-77
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2shqld3l
[[36m2022-10-23 07:31:06,014[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:31:06,019[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:31:06,020[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:31:06,021[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:31:06,026[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 75283.62it/s]
[[36m2022-10-23 07:31:06,133[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_351fe1 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_35205d │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 230.19it/s loss: 1.47e+03   
                                                               v_num: ld3l      
[[36m2022-10-23 07:42:00,391[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:42:00,391[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 62.783 MB of 62.783 MB uploaded (0.000 MB deduped)wandb: \ 62.783 MB of 62.783 MB uploaded (0.000 MB deduped)wandb: | 62.783 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: / 62.786 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: - 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: \ 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: | 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: / 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: - 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: \ 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: | 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▄▂▆▄▁▆▃█▅▂▇▅▂▇▄▁▆▃█▅▃▇▅▂▇▄▂▆▄▁▆▃█
wandb:                 fdr ▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▆█▇███▇▃▆▇▇▆▆▆▁█▁▁█▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▄▅▇██▇▇█▇▆▆▅▅▃▄▁▂▃▁▂▂▂▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:                 tpr ▄▃▅▅▇▄▄▆█▇▆▅▅▆▆▆▂▁▂▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▄▁▂▁▁▁▁▂▂▆▁▂▂█▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.0028
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1993.48792
wandb: trainer/global_step 104999
wandb: 
wandb: Synced volcanic-spaceship-77: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2shqld3l
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_073101-2shqld3l/logs
[[36m2022-10-23 07:42:05,412[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/11[0m
[[36m2022-10-23 07:42:05,412[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:42:05,417[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:42:05,417[0m][[35mHYDRA[0m] 	#12 : trainer.max_epochs=300 model.optimizer.lr=0.0017053336568535853 model.eta=8.370741811450799 model.gamma=0.7805833696113673 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:42:05,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:42:05,532[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0017053336568535853                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 8.370741811450799                                                  
│       gamma: 0.7805833696113673                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 07:42:05,558[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:42:05,559[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:42:05,560[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:42:05,576[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:42:05,576[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:42:05,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:42:05,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:42:05,578[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:42:05,578[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:42:05,578[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_074205-ycvq9d7d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-haze-78
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/ycvq9d7d
[[36m2022-10-23 07:42:10,192[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:42:10,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:42:10,196[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:42:10,197[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:42:10,203[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 76%|███████▌  | 5661/7466 [00:00<00:00, 30108.08it/s]100%|██████████| 7466/7466 [00:00<00:00, 34922.48it/s]
[[36m2022-10-23 07:42:10,424[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_c0ed47 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_c0edc0 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 224.87it/s loss: 168 v_num: 
                                                               9d7d             
[[36m2022-10-23 07:53:16,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:53:16,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 54.411 MB of 54.425 MB uploaded (0.000 MB deduped)wandb: \ 54.411 MB of 54.425 MB uploaded (0.000 MB deduped)wandb: | 54.411 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▇▄▂▇▄▁▆▃█▅▃▇▆▄▁▆▃█▅▂▇▅▂▇▄▃█▅▂▇▄▂▆▄▁▆▃█▇
wandb:                 fdr ▇▇▇▆▇▇▇▇▇▆▇▇▇▆▆▇▇▇▇▇▇██▇██████▁▁▁▁▁▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd █▄▄▄▄▆▇▇▆▅▆▄▄▂▂▃▃▃▃▂▂▂▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 tpr ▃▅▆█▆▅▅▃▃▆▅▃▃▃▅▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▂▁▁▁▁▁▁▂▁▁▂▁▂▁▃▁▁▁▁█▁▁▁▅▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00171
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 198.38333
wandb: trainer/global_step 109199
wandb: 
wandb: Synced breezy-haze-78: https://wandb.ai/flpie/DAG_GCN_optuna/runs/ycvq9d7d
wandb: Synced 6 W&B file(s), 1612 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_074205-ycvq9d7d/logs
[[36m2022-10-23 07:54:54,690[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/12[0m
[[36m2022-10-23 07:54:54,690[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:54:54,695[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:54:54,695[0m][[35mHYDRA[0m] 	#13 : trainer.max_epochs=300 model.optimizer.lr=0.00467193103159237 model.eta=2.336688963322965 model.gamma=0.9976062012462577 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:54:54,806[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:54:54,807[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.00467193103159237                                               
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 2.336688963322965                                                  
│       gamma: 0.9976062012462577                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 07:54:54,834[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:54:54,834[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:54:54,835[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:54:54,851[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:54:54,851[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:54:54,852[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_075454-2x8g4ore
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-fog-79
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2x8g4ore
[[36m2022-10-23 07:54:59,353[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:54:59,359[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:54:59,359[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:54:59,360[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:54:59,367[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 97%|█████████▋| 7233/7466 [00:00<00:00, 72327.31it/s]100%|██████████| 7466/7466 [00:00<00:00, 72110.59it/s]
[[36m2022-10-23 07:54:59,484[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_8b7355 │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_8b73cd │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 218.64it/s loss: 2.45e+03   
                                                               v_num: 4ore      
[[36m2022-10-23 08:05:40,498[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 08:05:40,498[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 69.083 MB of 69.083 MB uploaded (0.000 MB deduped)wandb: \ 69.083 MB of 69.083 MB uploaded (0.000 MB deduped)wandb: | 69.083 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: / 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: - 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: \ 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: | 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: / 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: - 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: \ 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: | 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: / 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▃▇▅▂▇▄▁▅▂▇▄▁▆▂▇▄▁▆▃█▄▁▆▃█▅▁▆▃█▅▂▇
wandb:                 fdr ▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▆▅▆▇▆▇▅▇▆▆▆▄▅▅▅▅▇█▁▁▁█
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▅▄▆▇▆█▆▇█▆▆▇▇▆█▆▄▄▃▂▃▃▃▃▁▄▂▃▂▁▁▁▁▁▃▂▁▁▁▂
wandb:                 tpr ▂▄▇▅▆▅█▆▄▄▄▅▅▄▃▃▂▃▃▂▄▂▂▂▂▂▂▃▁▃▂▂▁▁▂▁▁▁▁▁
wandb:          train/loss ▁▁▂▂▁▁▁▂▅▁█▂▁▂▂▃▂▂▅▂▂▁▃▁▁▂▂▂▁▂▂▂▂▁▃▁▁▁▁▂
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00467
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1848.41663
wandb: trainer/global_step 100799
wandb: 
wandb: Synced sunny-fog-79: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2x8g4ore
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_075454-2x8g4ore/logs
[[36m2022-10-23 08:05:45,268[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/13[0m
[[36m2022-10-23 08:05:45,268[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 08:05:45,273[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 08:05:45,273[0m][[35mHYDRA[0m] 	#14 : trainer.max_epochs=300 model.optimizer.lr=0.0021503938144996396 model.eta=13.500535599345383 model.gamma=0.2815666977596547 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 08:05:45,537[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 08:05:45,538[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: false                                                       
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: false                                                     
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.0021503938144996396                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 13.500535599345383                                                 
│       gamma: 0.2815666977596547                                               
│       graph_threshold: 0.3                                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── k_max_iter
│   └── 10                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 300                                                         
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-23 08:05:45,566[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 08:05:45,566[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 08:05:45,567[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 08:05:45,583[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 08:05:45,583[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 08:05:45,584[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_080545-25z09gb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-butterfly-80
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/25z09gb6
[[36m2022-10-23 08:05:49,980[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 08:05:49,983[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 08:05:49,983[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 08:05:49,983[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 08:05:49,984[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 08:05:49,984[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 08:05:49,984[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 08:05:49,985[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 08:05:49,994[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 77235.70it/s]
[[36m2022-10-23 08:05:50,098[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                       ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                    │ DAG_GCN_Encoder   │  8.6 K │
│ 1  │ encoder.model              │ Sequential_0f512d │  8.5 K │
│ 2  │ encoder.model.module_0     │ Linear            │    128 │
│ 3  │ encoder.model.module_1     │ LeakyReLU         │      0 │
│ 4  │ encoder.model.module_2     │ Linear            │  4.2 K │
│ 5  │ encoder.model.module_3     │ DenseGCNConv      │  4.2 K │
│ 6  │ encoder.model.module_3.lin │ Linear            │  4.1 K │
│ 7  │ encoder.model.module_4     │ LeakyReLU         │      0 │
│ 8  │ encoder.model.module_5     │ DenseGCNConv      │     65 │
│ 9  │ encoder.model.module_5.lin │ Linear            │     64 │
│ 10 │ decoder                    │ DAG_GCN_Decoder   │  8.5 K │
│ 11 │ decoder.model              │ Sequential_0f51a8 │  8.5 K │
│ 12 │ decoder.model.module_0     │ DenseGCNConv      │    128 │
│ 13 │ decoder.model.module_0.lin │ Linear            │     64 │
│ 14 │ decoder.model.module_1     │ LeakyReLU         │      0 │
│ 15 │ decoder.model.module_2     │ DenseGCNConv      │  4.2 K │
│ 16 │ decoder.model.module_2.lin │ Linear            │  4.1 K │
│ 17 │ decoder.model.module_3     │ Linear            │  4.2 K │
│ 18 │ decoder.model.module_4     │ LeakyReLU         │      0 │
│ 19 │ decoder.model.module_5     │ Linear            │     65 │
└────┴────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 220.29it/s loss: 233 v_num: 
                                                               9gb6             
[[36m2022-10-23 08:16:40,691[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 08:16:40,691[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 57.122 MB of 57.122 MB uploaded (0.000 MB deduped)wandb: \ 57.122 MB of 57.122 MB uploaded (0.000 MB deduped)wandb: | 57.122 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: / 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: - 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: \ 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: | 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: / 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: - 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: \ 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: | 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▂▅▃▇▅▂▇▄▂▆▄▁▆▃█▅▂▇▅▂▇▄▁▆▃█▅▃▇▅▂▇▄▂▆▄▁▆▃█
wandb:                 fdr █▆▇▆▇▇▇▇▇█████▇██▇▇▇▇███████▆▇▁███▁▁▁▁▁▁
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ▅▃▃▂▄▅▅▅▆█▇▆▄▄▃▄▄▂▂▂▂▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁
wandb:                 tpr ▂▅▅█▆▇▅▅▆▃▂▂▂▁▂▂▂▃▂▃▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss ▂▁▂▁▁▂▁▄▁▇▂▁▂█▂▁▂▁▃▁▂▂▁▁▂▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00215
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 317.24094
wandb: trainer/global_step 104999
wandb: 
wandb: Synced winter-butterfly-80: https://wandb.ai/flpie/DAG_GCN_optuna/runs/25z09gb6
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_080545-25z09gb6/logs
[[36m2022-10-23 08:16:45,703[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/14[0m
[[36m2022-10-23 08:16:45,703[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 08:16:45,704[0m][[35mHYDRA[0m] Best parameters: {'trainer.max_epochs': 300, 'model.optimizer.lr': 0.0019960425587510337, 'model.eta': 12.820066649756805, 'model.gamma': 0.493954965106403, 'model.activation': 'torch.nn.LeakyReLU'}[0m
[[36m2022-10-23 08:16:45,704[0m][[35mHYDRA[0m] Best value: 0.0[0m
[32m[I 2022-10-24 03:57:27,780][0m A new study created in memory with name: no-name-5697e595-cda4-44c1-b447-5b7d743702a4[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Study name: no-name-5697e595-cda4-44c1-b447-5b7d743702a4[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Directions: ['maximize'][0m
[[36m2022-10-24 03:57:27,783[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 03:57:27,783[0m][[35mHYDRA[0m] 	#0 : model.graph_threshold=0.17660778015155693 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 03:57:28,663[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 03:57:28,667[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.17660778015155693                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 03:57:28,782[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 03:57:28,782[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 03:57:28,916[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
[[36m2022-10-24 03:57:29,026[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpe4mei9q6[0m
[[36m2022-10-24 03:57:29,026[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpe4mei9q6/_remote_module_non_scriptable.py[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 03:57:29,044[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 03:57:29,044[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 03:57:29,046[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 03:57:29,046[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 03:57:29,047[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 03:57:29,047[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 03:57:29,047[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: flpie. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_035730-2p899k9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-pine-106
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2p899k9h
[[36m2022-10-24 03:57:33,945[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 03:57:33,949[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 03:57:33,950[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 03:57:33,957[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 85%|████████▍ | 6338/7466 [00:00<00:00, 63372.05it/s]100%|██████████| 7466/7466 [00:00<00:00, 63341.30it/s]
[[36m2022-10-24 03:57:34,769[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_8ab0cf │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_8ab167 │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('train/accu', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('shd', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/acc')` could not find the monitored key in the returned metrics: ['train/loss', 'train/losses', 'train/accu', 'shd', 'tpr', 'fdr', 'epoch', 'step']. HINT: Did you call `log('val/acc', value)` in the `LightningModule`?
  warning_cache.warn(m)
Exception in thread MsgRouterThr:
Traceback (most recent call last):
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 69, in message_loop
    msg = self._read_message()
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py", line 32, in _read_message
    msg = self._response_queue.get(timeout=1)
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/queues.py", line 117, in get
    res = self._recv_bytes()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 217, in recv_bytes
    self._check_closed()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 141, in _check_closed
    raise OSError("handle is closed")
OSError: handle is closed
[32m[I 2022-10-24 03:59:34,817][0m A new study created in memory with name: no-name-453d9238-e7d0-4c01-96a5-a93d6df979a2[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Study name: no-name-453d9238-e7d0-4c01-96a5-a93d6df979a2[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Directions: ['maximize'][0m
[[36m2022-10-24 03:59:34,819[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 03:59:34,819[0m][[35mHYDRA[0m] 	#0 : model.graph_threshold=0.17660778015155693 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 03:59:35,684[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 03:59:35,689[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.17660778015155693                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 03:59:35,799[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 03:59:35,800[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 03:59:35,934[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
[[36m2022-10-24 03:59:36,045[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpug6gabfw[0m
[[36m2022-10-24 03:59:36,045[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpug6gabfw/_remote_module_non_scriptable.py[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 03:59:36,063[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 03:59:36,064[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 03:59:36,066[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 03:59:36,066[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 03:59:36,066[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 03:59:36,067[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 03:59:36,067[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: flpie. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_035937-3dyl6otg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-dawn-107
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3dyl6otg
[[36m2022-10-24 03:59:40,862[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 03:59:40,866[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 03:59:40,867[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 03:59:40,868[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 03:59:40,875[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 88%|████████▊ | 6538/7466 [00:00<00:00, 65182.85it/s]100%|██████████| 7466/7466 [00:00<00:00, 64763.70it/s]
[[36m2022-10-24 03:59:41,685[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_d66668 │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_d66702 │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('train/accu', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('shd', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/acc')` could not find the monitored key in the returned metrics: ['train/loss', 'train/losses', 'train/accu', 'shd', 'tpr', 'fdr', 'epoch', 'step']. HINT: Did you call `log('val/acc', value)` in the `LightningModule`?
  warning_cache.warn(m)
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 188.69it/s loss: 7.83e+03   
                                                               v_num: 6otg      
[[36m2022-10-24 04:41:39,840[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 04:41:39,840[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 241.541 MB of 241.541 MB uploaded (0.000 MB deduped)wandb: \ 241.541 MB of 241.541 MB uploaded (0.000 MB deduped)wandb: | 241.541 MB of 241.541 MB uploaded (0.000 MB deduped)wandb: / 241.541 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: - 241.541 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: \ 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: | 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: / 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: - 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: \ 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: | 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: / 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: - 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: \ 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▄▄▄▄▃▃▂▂▁▂▁▁▂▁▁▂▁▁▃▂▃▃▄▄▃▃▄▅▄▃▁▅████████
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ██▇▆▅▅▄▄▃▄▃▄▃▃▃▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▂▂▁▁▁▂▂▁▁
wandb:                 tpr ██▇▆▅▅▅▆▅▅▆▆▅▆▆▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▃▂▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 1.0
wandb:             lr-Adam 0.001
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced wandering-dawn-107: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3dyl6otg
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_035937-3dyl6otg/logs
[[36m2022-10-24 04:41:45,581[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/0[0m
[[36m2022-10-24 04:41:45,581[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 04:41:45,584[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 04:41:45,584[0m][[35mHYDRA[0m] 	#1 : model.graph_threshold=0.34884350841593276 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 04:41:45,698[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 04:41:45,699[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.34884350841593276                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 04:41:45,728[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 04:41:45,728[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 04:41:45,729[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 04:41:45,749[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 04:41:45,749[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 04:41:45,752[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_044145-2l7tie1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sky-108
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2l7tie1o
[[36m2022-10-24 04:41:50,431[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 04:41:50,434[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 04:41:50,434[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 04:41:50,434[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 04:41:50,435[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 04:41:50,435[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 04:41:50,435[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 04:41:50,436[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 04:41:50,443[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 96%|█████████▌| 7176/7466 [00:00<00:00, 71752.54it/s]100%|██████████| 7466/7466 [00:00<00:00, 71706.70it/s]
[[36m2022-10-24 04:41:50,555[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_ba380e │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_ba389f │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 179.33it/s loss: 7.83e+03   
                                                               v_num: ie1o      
[[36m2022-10-24 05:24:10,464[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 05:24:10,464[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 189.695 MB of 189.695 MB uploaded (0.000 MB deduped)wandb: \ 189.695 MB of 189.695 MB uploaded (0.000 MB deduped)wandb: | 189.695 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: / 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: - 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: \ 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: | 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: / 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: - 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: \ 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: | 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▇▇▇▇▇▇▇▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▆████████████▁█
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd █▇▇▆▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁
wandb:                 tpr █▇▆▅▄▃▃▄▄▄▄▄▄▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced neat-sky-108: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2l7tie1o
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_044145-2l7tie1o/logs
[[36m2022-10-24 05:24:15,048[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/1[0m
[[36m2022-10-24 05:24:15,048[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 05:24:15,051[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 05:24:15,051[0m][[35mHYDRA[0m] 	#2 : model.graph_threshold=0.27509109560284584 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 05:24:15,166[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 05:24:15,167[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.27509109560284584                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 05:24:15,197[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 05:24:15,197[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 05:24:15,198[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 05:24:15,218[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 05:24:15,218[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 05:24:15,220[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 05:24:15,220[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 05:24:15,220[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 05:24:15,221[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 05:24:15,221[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_052415-4tluxkzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-eon-109
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/4tluxkzv
[[36m2022-10-24 05:24:19,835[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 05:24:19,838[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 05:24:19,839[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 05:24:19,846[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 78265.16it/s]
[[36m2022-10-24 05:24:19,950[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_a9d225 │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_a9d2b3 │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 178.64it/s loss: 7.83e+03   
                                                               v_num: xkzv      
[[36m2022-10-24 06:07:32,076[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 06:07:32,077[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 205.941 MB of 205.941 MB uploaded (0.000 MB deduped)wandb: \ 205.941 MB of 205.941 MB uploaded (0.000 MB deduped)wandb: | 205.941 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: / 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: - 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: \ 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: | 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: / 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: - 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: \ 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▇▇▇▇▇▇▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████▁█
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd █▇▇▆▄▄▃▃▃▃▃▃▃▃▃▄▄▃▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁
wandb:                 tpr █▇▆▅▅▃▄▅▅▅▅▄▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced glamorous-eon-109: https://wandb.ai/flpie/DAG_GCN_optuna/runs/4tluxkzv
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_052415-4tluxkzv/logs
[[36m2022-10-24 06:07:37,900[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/2[0m
[[36m2022-10-24 06:07:37,900[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 06:07:37,903[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 06:07:37,903[0m][[35mHYDRA[0m] 	#3 : model.graph_threshold=0.41414343348550775 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 06:07:38,020[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 06:07:38,021[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.41414343348550775                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 06:07:38,049[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 06:07:38,049[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 06:07:38,051[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 06:07:38,071[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 06:07:38,071[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 06:07:38,072[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 06:07:38,073[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 06:07:38,073[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 06:07:38,073[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 06:07:38,074[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_060738-l5xmhom6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-shape-110
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/l5xmhom6
[[36m2022-10-24 06:07:42,681[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 06:07:42,684[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 06:07:42,685[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 06:07:42,686[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 06:07:42,693[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 77889.06it/s]
[[36m2022-10-24 06:07:42,797[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_b93de1 │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_b93e7d │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 170.20it/s loss: 7.83e+03   
                                                               v_num: hom6      
[[36m2022-10-24 06:50:30,710[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 06:50:30,710[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 180.493 MB of 180.493 MB uploaded (0.000 MB deduped)wandb: \ 180.493 MB of 180.493 MB uploaded (0.000 MB deduped)wandb: | 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: / 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: - 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: \ 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: | 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: / 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: - 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: \ 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: | 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▇▇▇▇▇▇▆▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████▁▁█
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ██▇▆▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb:                 tpr █▇▇▅▄▂▃▃▃▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced peachy-shape-110: https://wandb.ai/flpie/DAG_GCN_optuna/runs/l5xmhom6
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_060738-l5xmhom6/logs
[[36m2022-10-24 06:50:35,294[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/3[0m
[[36m2022-10-24 06:50:35,294[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 06:50:35,299[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 06:50:35,299[0m][[35mHYDRA[0m] 	#4 : model.graph_threshold=0.4119903232475214 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 06:50:35,413[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 06:50:35,414[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.4119903232475214                                     
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 06:50:35,443[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 06:50:35,443[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 06:50:35,444[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 06:50:35,463[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 06:50:35,463[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 06:50:35,465[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 06:50:35,465[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 06:50:35,466[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 06:50:35,466[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 06:50:35,466[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_065035-2atk5yc7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-rain-111
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2atk5yc7
[[36m2022-10-24 06:50:40,133[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 06:50:40,136[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 06:50:40,136[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 06:50:40,137[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 06:50:40,137[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 06:50:40,137[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 06:50:40,137[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 06:50:40,138[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 06:50:40,143[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 96%|█████████▋| 7203/7466 [00:00<00:00, 72025.95it/s]100%|██████████| 7466/7466 [00:00<00:00, 72156.95it/s]
[[36m2022-10-24 06:50:40,255[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_b97ccf │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_b97d57 │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 183.34it/s loss: 7.83e+03   
                                                               v_num: 5yc7      
[[36m2022-10-24 07:33:47,199[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 07:33:47,199[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 180.701 MB of 180.701 MB uploaded (0.000 MB deduped)wandb: \ 180.701 MB of 180.701 MB uploaded (0.000 MB deduped)wandb: | 180.701 MB of 180.701 MB uploaded (0.000 MB deduped)wandb: / 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: - 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: \ 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: | 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: / 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: - 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: \ 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: | 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▇▇▇▇▇▇▆▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████▁▁█
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ██▇▆▅▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁
wandb:                 tpr █▇▇▅▄▂▃▃▃▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced fallen-rain-111: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2atk5yc7
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_065035-2atk5yc7/logs
[[36m2022-10-24 07:33:51,955[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/4[0m
[[36m2022-10-24 07:33:51,955[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 07:33:51,959[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 07:33:51,959[0m][[35mHYDRA[0m] 	#5 : model.graph_threshold=0.11182750899709085 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 07:33:52,071[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 07:33:52,072[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.11182750899709085                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 07:33:52,100[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 07:33:52,100[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 07:33:52,101[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 07:33:52,121[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 07:33:52,121[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 07:33:52,123[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 07:33:52,123[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 07:33:52,123[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 07:33:52,124[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 07:33:52,124[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_073352-d47ykhg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-fire-112
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/d47ykhg1
[[36m2022-10-24 07:33:56,695[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 07:33:56,699[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 07:33:56,700[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 07:33:56,706[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 75736.81it/s]
[[36m2022-10-24 07:33:56,813[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_c53752 │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_c537e2 │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 177.36it/s loss: 7.83e+03   
                                                               v_num: khg1      
[[36m2022-10-24 08:18:46,359[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 08:18:46,359[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 282.346 MB of 282.346 MB uploaded (0.000 MB deduped)wandb: \ 282.346 MB of 282.346 MB uploaded (0.000 MB deduped)wandb: | 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: / 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: - 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: \ 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: | 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: / 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▄▄▄▄▃▂▂▃▂▂▃▃▃▃▃▃▃▃▂▃▂▃▄▄▃▅▃▄▃▁▃▄▃▆▂▃▆██▅
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd █▇▇▆▅▅▄▅▄▄▅▄▅▅▃▄▄▄▃▄▃▃▃▃▃▃▂▂▂▁▂▂▁▁▁▁▂▂▁▁
wandb:                 tpr █▇▇▅▆▇▆▆▆▆▆▆▆▆▅▆▆▆▆▅▆▄▄▃▄▃▃▃▃▄▃▃▃▂▂▂▂▁▁▂
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 1.0
wandb:             lr-Adam 0.001
wandb:                 shd 23.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced absurd-fire-112: https://wandb.ai/flpie/DAG_GCN_optuna/runs/d47ykhg1
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_073352-d47ykhg1/logs
[[36m2022-10-24 08:18:51,419[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/5[0m
[[36m2022-10-24 08:18:51,420[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 08:18:51,423[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 08:18:51,423[0m][[35mHYDRA[0m] 	#6 : model.graph_threshold=0.16292055445901682 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 08:18:51,537[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 08:18:51,538[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.16292055445901682                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 08:18:51,566[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 08:18:51,567[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 08:18:51,568[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 08:18:51,587[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 08:18:51,587[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 08:18:51,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 08:18:51,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 08:18:51,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 08:18:51,590[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 08:18:51,590[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_081851-3f7pmq2v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sunset-113
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3f7pmq2v
[[36m2022-10-24 08:18:56,151[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 08:18:56,156[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 08:18:56,156[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 08:18:56,162[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 80155.72it/s]
[[36m2022-10-24 08:18:56,264[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_0e3947 │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_0e39d9 │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 165.46it/s loss: 7.83e+03   
                                                               v_num: mq2v      
[[36m2022-10-24 09:03:37,622[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 09:03:37,622[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 249.385 MB of 249.385 MB uploaded (0.000 MB deduped)wandb: \ 249.385 MB of 249.385 MB uploaded (0.000 MB deduped)wandb: | 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: / 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: - 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: \ 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: | 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: / 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▄▄▄▄▃▂▂▂▂▂▁▁▂▁▂▂▂▂▃▂▂▂▄▄▄▃▅▅▃▃▂▆████▄███
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd ██▇▆▅▄▄▄▄▄▃▄▄▃▃▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▂▂▁▁▁▁▂▁▁
wandb:                 tpr ██▇▆▅▅▅▆▅▅▆▆▅▆▆▆▅▅▄▅▄▄▃▃▃▃▂▂▃▂▃▂▁▁▁▁▂▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 1.0
wandb:             lr-Adam 0.001
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced wild-sunset-113: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3f7pmq2v
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_081851-3f7pmq2v/logs
[[36m2022-10-24 09:03:41,986[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/6[0m
[[36m2022-10-24 09:03:41,986[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 09:03:41,990[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 09:03:41,990[0m][[35mHYDRA[0m] 	#7 : model.graph_threshold=0.22958692509211015 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 09:03:42,101[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 09:03:42,102[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.22958692509211015                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 09:03:42,129[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 09:03:42,129[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 09:03:42,130[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 09:03:42,149[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 09:03:42,150[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 09:03:42,151[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_090342-30jmio8r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-rain-114
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/30jmio8r
[[36m2022-10-24 09:03:46,668[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 09:03:46,671[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 09:03:46,671[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 09:03:46,672[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 09:03:46,672[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 09:03:46,672[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 09:03:46,672[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 09:03:46,673[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 09:03:46,679[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 98%|█████████▊| 7338/7466 [00:00<00:00, 73369.75it/s]100%|██████████| 7466/7466 [00:00<00:00, 73264.92it/s]
[[36m2022-10-24 09:03:46,789[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_51ec7d │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_51ed0b │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 ━━━━━━━━━━━━━━━━━ 14/14 0:00:00 • 0:00:00 182.31it/s loss: 7.83e+03   
                                                               v_num: io8r      
[[36m2022-10-24 09:47:55,041[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 09:47:55,041[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 218.282 MB of 218.282 MB uploaded (0.000 MB deduped)wandb: \ 218.282 MB of 218.282 MB uploaded (0.000 MB deduped)wandb: | 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: / 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: - 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: \ 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: | 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: / 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▃▆▃▁▅▃▇▅▂▇▄▂▆▄█▄▃▆▅▁▇▃▁▅▃▇▅▂▆▄█▆▂█▄▃▆▅▁▇
wandb:                 fdr ▅▄▅▅▄▄▂▃▂▂▂▃▃▃▃▃▄▄▆▆▅▅▄▄▃▅▄▂▆▄▁█████████
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 shd █▇▇▆▄▄▃▃▃▃▃▃▃▃▃▃▄▃▄▄▃▃▂▂▂▂▂▁▂▁▁▂▂▁▁▁▂▂▁▁
wandb:                 tpr ██▇▅▅▄▅▅▅▅▅▅▅▅▅▅▃▃▂▂▃▃▃▃▃▂▂▂▂▂▃▁▁▁▁▁▁▁▁▁
wandb:          train/loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced stellar-rain-114: https://wandb.ai/flpie/DAG_GCN_optuna/runs/30jmio8r
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_090342-30jmio8r/logs
[[36m2022-10-24 09:47:59,774[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/7[0m
[[36m2022-10-24 09:47:59,774[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 09:47:59,778[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 09:47:59,778[0m][[35mHYDRA[0m] 	#8 : model.graph_threshold=0.20962010369362757 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 09:47:59,891[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 09:47:59,892[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
├── datamodule
│   └── _target_: src.datamodules.sachs_datamodule.SachsDataModule              
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       batch_size: 512                                                         
│       shuffle: true                                                           
│                                                                               
├── model
│   └── _target_: src.models.DAG_GCN.DAG_GCN                                    
│       activation: torch.nn.LeakyReLU                                          
│       batch_norm: true                                                        
│       init: true                                                              
│       encoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
│         in_channels: 1                                                        
│         H1: 64                                                                
│         H2: 64                                                                
│         H3: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       decoder:                                                                
│         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
│         in_channels: 1                                                        
│         H3: 64                                                                
│         H2: 64                                                                
│         H1: 64                                                                
│         out_channels: 1                                                       
│         activation:                                                           
│           _target_: torch.nn.LeakyReLU                                        
│         batch_norm: true                                                      
│         num_features: 11                                                      
│       optimizer:                                                              
│         _target_: torch.optim.Adam                                            
│         _partial_: true                                                       
│         lr: 0.001                                                             
│         weight_decay: 0.0                                                     
│       num_features: 11                                                        
│       batch_size: 512                                                         
│       lambda_A: 0.0                                                           
│       c_A: 1                                                                  
│       eta: 10                                                                 
│       gamma: 0.25                                                             
│       graph_threshold: 0.20962010369362757                                    
│       adj_high: 0.1                                                           
│       adj_low: -0.1                                                           
│       plot_every: 10                                                          
│       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── callbacks
│   └── model_checkpoint:                                                       
│         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
│         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
│         filename: epoch_{epoch:03d}                                           
│         monitor: val/acc                                                      
│         verbose: false                                                        
│         save_last: true                                                       
│         save_top_k: 1                                                         
│         mode: max                                                             
│         auto_insert_metric_name: false                                        
│         save_weights_only: false                                              
│         every_n_train_steps: null                                             
│         train_time_interval: null                                             
│         every_n_epochs: null                                                  
│         save_on_train_epoch_end: null                                         
│       early_stopping: false                                                   
│       model_summary:                                                          
│         _target_: pytorch_lightning.callbacks.RichModelSummary                
│         max_depth: -1                                                         
│       rich_progress_bar:                                                      
│         _target_: pytorch_lightning.callbacks.RichProgressBar                 
│       learning_rate_monitor:                                                  
│         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
│         logging_interval: epoch                                               
│                                                                               
├── logger
│   └── wandb:                                                                  
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
│         save_dir: .                                                           
│         offline: false                                                        
│         id: null                                                              
│         anonymous: null                                                       
│         project: DAG_GCN_optuna                                               
│         log_model: false                                                      
│         prefix: ''                                                            
│         group: ''                                                             
│         tags: []                                                              
│         job_type: ''                                                          
│                                                                               
├── trainer
│   └── _target_: pytorch_lightning.Trainer                                     
│       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       limit_val_batches: 0                                                    
│       log_every_n_steps: 14                                                   
│       accelerator: gpu                                                        
│       devices: 1                                                              
│       deterministic: false                                                    
│                                                                               
├── paths
│   └── root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
│       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
│       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
│                                                                               
├── extras
│   └── ignore_warnings: false                                                  
│       enforce_tags: true                                                      
│       print_config: true                                                      
│                                                                               
├── task_name
│   └── train                                                                   
├── tags
│   └── ['sachs', 'DAG_GCN']                                                    
├── train
│   └── True                                                                    
├── test
│   └── False                                                                   
├── ckpt_path
│   └── None                                                                    
├── seed
│   └── 42                                                                      
├── loop
│   └── _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
│       min_epochs: 1                                                           
│       max_epochs: 1000                                                        
│       k_max_iter: 10                                                          
│                                                                               
└── optimized_metric
    └── tpr                                                                     
[[36m2022-10-24 09:47:59,920[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 09:47:59,920[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 09:47:59,921[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 09:47:59,940[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 09:47:59,940[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 09:47:59,942[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 09:47:59,942[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 09:47:59,943[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 09:47:59,943[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 09:47:59,943[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_094759-1esh7rvl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-eon-115
wandb: ⭐️ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: 🚀 View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/1esh7rvl
[[36m2022-10-24 09:48:04,563[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 09:48:04,568[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 09:48:04,569[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 09:48:04,570[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 09:48:04,576[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|██████████| 7466/7466 [00:00<00:00, 75633.64it/s]
[[36m2022-10-24 09:48:04,683[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                          ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ encoder                       │ DAG_GCN_Encoder   │  8.9 K │
│ 1  │ encoder.model                 │ Sequential_821734 │  8.8 K │
│ 2  │ encoder.model.module_0        │ Linear            │    128 │
│ 3  │ encoder.model.module_1        │ BatchNorm         │    128 │
│ 4  │ encoder.model.module_1.module │ BatchNorm1d       │    128 │
│ 5  │ encoder.model.module_2        │ LeakyReLU         │      0 │
│ 6  │ encoder.model.module_3        │ Linear            │  4.2 K │
│ 7  │ encoder.model.module_4        │ BatchNorm         │    128 │
│ 8  │ encoder.model.module_4.module │ BatchNorm1d       │    128 │
│ 9  │ encoder.model.module_5        │ LeakyReLU         │      0 │
│ 10 │ encoder.model.module_6        │ DenseGCNConv      │  4.2 K │
│ 11 │ encoder.model.module_6.lin    │ Linear            │  4.1 K │
│ 12 │ encoder.model.module_7        │ BatchNorm         │     22 │
│ 13 │ encoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 14 │ encoder.model.module_8        │ LeakyReLU         │      0 │
│ 15 │ encoder.model.module_9        │ DenseGCNConv      │     65 │
│ 16 │ encoder.model.module_9.lin    │ Linear            │     64 │
│ 17 │ decoder                       │ DAG_GCN_Decoder   │  8.6 K │
│ 18 │ decoder.model                 │ Sequential_8217c1 │  8.6 K │
│ 19 │ decoder.model.module_0        │ DenseGCNConv      │    128 │
│ 20 │ decoder.model.module_0.lin    │ Linear            │     64 │
│ 21 │ decoder.model.module_1        │ BatchNorm         │     22 │
│ 22 │ decoder.model.module_1.module │ BatchNorm1d       │     22 │
│ 23 │ decoder.model.module_2        │ LeakyReLU         │      0 │
│ 24 │ decoder.model.module_3        │ DenseGCNConv      │  4.2 K │
│ 25 │ decoder.model.module_3.lin    │ Linear            │  4.1 K │
│ 26 │ decoder.model.module_4        │ BatchNorm         │     22 │
│ 27 │ decoder.model.module_4.module │ BatchNorm1d       │     22 │
│ 28 │ decoder.model.module_5        │ LeakyReLU         │      0 │
│ 29 │ decoder.model.module_6        │ Linear            │  4.2 K │
│ 30 │ decoder.model.module_7        │ BatchNorm         │     22 │
│ 31 │ decoder.model.module_7.module │ BatchNorm1d       │     22 │
│ 32 │ decoder.model.module_8        │ LeakyReLU         │      0 │
│ 33 │ decoder.model.module_9        │ Linear            │     65 │
└────┴───────────────────────────────┴───────────────────┴────────┘
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Exception in thread MsgRouterThr:
Traceback (most recent call last):
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 69, in message_loop
    msg = self._read_message()
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py", line 32, in _read_message
    msg = self._response_queue.get(timeout=1)
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/queues.py", line 117, in get
    res = self._recv_bytes()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 217, in recv_bytes
    self._check_closed()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 141, in _check_closed
    raise OSError("handle is closed")
OSError: handle is closed
