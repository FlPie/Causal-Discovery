[32m[I 2022-10-23 05:31:44,667][0m A new study created in memory with name: no-name-c8694c0a-3f3c-45b8-82e9-e92e82f69c94[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Study name: no-name-c8694c0a-3f3c-45b8-82e9-e92e82f69c94[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2022-10-23 05:31:44,667[0m][[35mHYDRA[0m] Directions: ['maximize'][0m
[[36m2022-10-23 05:31:44,670[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 05:31:44,670[0m][[35mHYDRA[0m] 	#0 : trainer.max_epochs=300 model.optimizer.lr=0.0019960425587510337 model.eta=12.820066649756805 model.gamma=0.493954965106403 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 05:31:45,547[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 05:31:45,550[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0019960425587510337                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 12.820066649756805                                                 
â”‚       gamma: 0.493954965106403                                                
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 05:31:45,584[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 05:31:45,585[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 05:31:45,785[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
[[36m2022-10-23 05:31:45,895[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmp0574k1u4[0m
[[36m2022-10-23 05:31:45,895[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmp0574k1u4/_remote_module_non_scriptable.py[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 05:31:45,913[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 05:31:45,913[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 05:31:45,915[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 05:31:45,916[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: flpie. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_053147-3lq2l36n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-frost-66
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3lq2l36n
[[36m2022-10-23 05:31:51,433[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 05:31:51,436[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 05:31:51,437[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 05:31:51,437[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 05:31:51,438[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 05:31:51,444[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6417/7466 [00:00<00:00, 64164.71it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 64260.27it/s]
[[36m2022-10-23 05:31:52,231[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_8c08e2 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_8c0968 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('train/accu', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('shd', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/acc')` could not find the monitored key in the returned metrics: ['train/loss', 'train/losses', 'train/accu', 'shd', 'tpr', 'fdr', 'epoch', 'step']. HINT: Did you call `log('val/acc', value)` in the `LightningModule`?
  warning_cache.warn(m)
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 224.21it/s loss: 1.23e+03   
                                                               v_num: l36n      
[[36m2022-10-23 05:42:18,151[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 05:42:18,151[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 56.288 MB of 56.288 MB uploaded (0.000 MB deduped)wandb: \ 56.288 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: | 56.288 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: / 56.288 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: - 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: \ 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: | 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: / 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: - 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: \ 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: | 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb: / 56.312 MB of 56.312 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆ
wandb:                 fdr â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–â–â–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–‡â–†â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–†â–…â–ƒâ–„â–…â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–
wandb:                 tpr â–‚â–…â–ˆâ–ˆâ–„â–…â–‡â–„â–‡â–…â–…â–„â–„â–…â–‡â–‚â–„â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–‚â–â–
wandb:          train/loss â–ƒâ–â–â–â–â–â–â–ƒâ–â–„â–â–â–â–ˆâ–â–â–â–â–‚â–â–â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.002
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1659.35413
wandb: trainer/global_step 104999
wandb: 
wandb: Synced dulcet-frost-66: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3lq2l36n
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_053147-3lq2l36n/logs
[[36m2022-10-23 05:42:22,256[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/0[0m
[[36m2022-10-23 05:42:22,256[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 05:42:22,259[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 05:42:22,259[0m][[35mHYDRA[0m] 	#1 : trainer.max_epochs=300 model.optimizer.lr=0.002798666792298152 model.eta=6.252820847718837 model.gamma=0.8216849597815173 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 05:42:22,374[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 05:42:22,375[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.002798666792298152                                              
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 6.252820847718837                                                  
â”‚       gamma: 0.8216849597815173                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 05:42:22,402[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 05:42:22,402[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 05:42:22,403[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 05:42:22,419[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 05:42:22,419[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 05:42:22,421[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 05:42:22,422[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_054222-1d20456c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-firebrand-67
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/1d20456c
[[36m2022-10-23 05:42:27,008[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 05:42:27,015[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 05:42:27,016[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 05:42:27,016[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 05:42:27,022[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 77045.86it/s]
[[36m2022-10-23 05:42:27,127[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_076e91 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_076f0b â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 227.10it/s loss: 4.19e+03   
                                                               v_num: 456c      
[[36m2022-10-23 05:52:43,189[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 05:52:43,189[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 58.389 MB of 58.389 MB uploaded (0.000 MB deduped)wandb: \ 58.389 MB of 58.389 MB uploaded (0.000 MB deduped)wandb: | 58.389 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: / 58.389 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: - 58.389 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: \ 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: | 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: / 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: - 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: \ 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: | 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: / 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb: - 58.412 MB of 58.412 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–ƒâ–‡â–…â–‚â–‡â–„â–â–…â–‚â–‡â–„â–â–†â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–„â–â–†â–ƒâ–ˆâ–…â–â–†â–ƒâ–ˆâ–…â–‚â–‡
wandb:                 fdr â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–‡â–†â–†â–‡â–‡â–†â–…â–†â–†â–‡â–†â–…â–‡â–†â–â–â–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–…â–…â–‡â–†â–†â–ˆâ–†â–‡â–†â–†â–„â–…â–…â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–
wandb:                 tpr â–‚â–ƒâ–†â–…â–†â–…â–…â–…â–ˆâ–‡â–ˆâ–†â–„â–†â–„â–„â–ƒâ–„â–…â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–â–
wandb:          train/loss â–‚â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–‚â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.0028
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 3325.89307
wandb: trainer/global_step 100799
wandb: 
wandb: Synced trim-firebrand-67: https://wandb.ai/flpie/DAG_GCN_optuna/runs/1d20456c
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_054222-1d20456c/logs
[[36m2022-10-23 05:52:47,449[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/1[0m
[[36m2022-10-23 05:52:47,449[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 05:52:47,452[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 05:52:47,452[0m][[35mHYDRA[0m] 	#2 : trainer.max_epochs=300 model.optimizer.lr=0.0036423909725828802 model.eta=10.518907384945715 model.gamma=0.7151166416549226 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 05:52:47,563[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 05:52:47,564[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0036423909725828802                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10.518907384945715                                                 
â”‚       gamma: 0.7151166416549226                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 05:52:47,591[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 05:52:47,591[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 05:52:47,592[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 05:52:47,608[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 05:52:47,608[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 05:52:47,610[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 05:52:47,610[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 05:52:47,610[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 05:52:47,611[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 05:52:47,611[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_055247-3ahkpve2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-pine-68
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahkpve2
[[36m2022-10-23 05:52:52,192[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 05:52:52,199[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 05:52:52,200[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 05:52:52,200[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 05:52:52,201[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 05:52:52,207[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5662/7466 [00:00<00:00, 30230.55it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 35314.22it/s]
[[36m2022-10-23 05:52:52,425[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_7c12d4 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_7c134f â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 218.52it/s loss: 874 v_num: 
                                                               pve2             
[[36m2022-10-23 06:03:34,886[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:03:34,886[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 62.316 MB of 62.316 MB uploaded (0.000 MB deduped)wandb: \ 62.316 MB of 62.316 MB uploaded (0.000 MB deduped)wandb: | 62.316 MB of 62.316 MB uploaded (0.000 MB deduped)wandb: / 62.316 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: - 62.316 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: \ 62.316 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: | 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: / 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: - 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: \ 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: | 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: / 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: - 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb: \ 62.339 MB of 62.339 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆ
wandb:                 fdr â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–†â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–‚â–†â–â–…â–ˆâ–…â–†â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–…â–…â–‡â–ˆâ–‡â–†â–‡â–…â–†â–†â–…â–…â–‡â–…â–„â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                 tpr â–‚â–„â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–„â–…â–‚â–‚â–â–â–â–‚â–‚â–„â–‚â–‚â–‚â–â–ƒâ–‚â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–â–â–ƒâ–â–â–‚â–‚â–‚â–‚â–ˆâ–‚â–â–ƒâ–‡â–‚â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00364
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1160.40161
wandb: trainer/global_step 104999
wandb: 
wandb: Synced spring-pine-68: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahkpve2
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_055247-3ahkpve2/logs
[[36m2022-10-23 06:03:39,795[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/2[0m
[[36m2022-10-23 06:03:39,795[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:03:39,798[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:03:39,798[0m][[35mHYDRA[0m] 	#3 : trainer.max_epochs=300 model.optimizer.lr=0.005655842242049688 model.eta=10.558580140848385 model.gamma=0.11239160463161402 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 06:03:39,908[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:03:39,909[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.GELU                                               
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.005655842242049688                                              
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10.558580140848385                                                 
â”‚       gamma: 0.11239160463161402                                              
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 06:03:39,935[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:03:39,935[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:03:39,936[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:03:39,951[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:03:39,952[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:03:39,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:03:39,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:03:39,953[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:03:39,954[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:03:39,954[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_060339-3ahearvr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-armadillo-69
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahearvr
[[36m2022-10-23 06:03:44,561[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:03:44,564[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:03:44,564[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:03:44,564[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:03:44,565[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:03:44,565[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:03:44,565[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:03:44,566[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:03:44,572[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 75222.13it/s]
[[36m2022-10-23 06:03:44,678[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_00e695 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_00e70a â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 225.01it/s loss: 1.96e+03   
                                                               v_num: arvr      
[[36m2022-10-23 06:14:44,430[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:14:44,431[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 75.561 MB of 75.561 MB uploaded (0.000 MB deduped)wandb: \ 75.561 MB of 75.561 MB uploaded (0.000 MB deduped)wandb: | 75.561 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: / 75.561 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: - 75.561 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: \ 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: | 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: / 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: - 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: \ 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: | 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb: / 75.584 MB of 75.584 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆ
wandb:                 fdr â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–‡â–‡â–‡â–…â–‡â–‡â–†â–ƒâ–…â–†â–†â–…â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–†â–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–†â–ˆâ–†â–†â–ƒâ–ƒâ–„â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–
wandb:                 tpr â–„â–ˆâ–‡â–‡â–‡â–‡â–†â–†â–…â–…â–†â–…â–„â–…â–…â–†â–„â–…â–‚â–‚â–‚â–„â–ƒâ–â–ƒâ–ƒâ–„â–â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–‚â–â–„â–‚â–‚â–â–â–‚â–â–ˆâ–ƒâ–‚â–‚â–†â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–â–‚â–â–‚â–â–â–ƒâ–â–â–â–â–â–â–„â–‚â–â–‚
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00566
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 2564.73486
wandb: trainer/global_step 104999
wandb: 
wandb: Synced solar-armadillo-69: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3ahearvr
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_060339-3ahearvr/logs
[[36m2022-10-23 06:14:49,103[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/3[0m
[[36m2022-10-23 06:14:49,103[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:14:49,106[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:14:49,106[0m][[35mHYDRA[0m] 	#4 : trainer.max_epochs=300 model.optimizer.lr=0.0037123712406235856 model.eta=12.69252739023638 model.gamma=0.16784311747867892 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 06:14:49,217[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:14:49,218[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.GELU                                               
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0037123712406235856                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 12.69252739023638                                                  
â”‚       gamma: 0.16784311747867892                                              
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 06:14:49,246[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:14:49,246[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:14:49,247[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:14:49,263[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:14:49,264[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:14:49,265[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:14:49,265[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:14:49,265[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:14:49,266[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:14:49,266[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_061449-zeseqxfj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-forest-70
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/zeseqxfj
[[36m2022-10-23 06:14:53,858[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:14:53,861[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:14:53,861[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:14:53,862[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:14:53,868[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 74914.77it/s]
[[36m2022-10-23 06:14:53,977[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_8fd75f â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_8fd7d9 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 224.13it/s loss: 692 v_num: 
                                                               qxfj             
[[36m2022-10-23 06:26:13,242[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:26:13,243[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 72.806 MB of 72.806 MB uploaded (0.000 MB deduped)wandb: \ 72.806 MB of 72.806 MB uploaded (0.000 MB deduped)wandb: | 72.806 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: / 72.806 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: - 72.821 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: \ 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: | 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: / 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: - 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: \ 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: | 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb: / 72.829 MB of 72.829 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–‡â–„â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–ƒâ–ˆâ–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–‡
wandb:                 fdr â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–‡â–…â–†â–…â–…â–…â–‡â–†â–‡â–†â–…â–ƒâ–„â–„â–†â–‡â–â–â–â–â–ˆâ–ˆâ–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ƒâ–…â–…â–ˆâ–ˆâ–‡â–ˆâ–‡â–†â–†â–…â–†â–…â–…â–ƒâ–ƒâ–ƒâ–„â–â–‚â–‚â–â–â–ƒâ–‚â–„â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–‚â–‚â–
wandb:                 tpr â–…â–…â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–†â–…â–†â–„â–…â–‡â–‡â–…â–†â–„â–…â–„â–„â–…â–…â–ƒâ–„â–ƒâ–‚â–ƒâ–‚â–…â–…â–‚â–‚â–â–‚â–‚â–â–â–â–
wandb:          train/loss â–ƒâ–â–â–â–â–ƒâ–„â–â–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–â–ƒâ–‚â–â–„â–‚â–‚â–‚â–†â–‚â–ƒâ–‚â–‚â–„â–â–‚â–â–â–â–â–ˆâ–‚â–â–‚â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00371
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 720.3266
wandb: trainer/global_step 109199
wandb: 
wandb: Synced dark-forest-70: https://wandb.ai/flpie/DAG_GCN_optuna/runs/zeseqxfj
wandb: Synced 6 W&B file(s), 1612 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_061449-zeseqxfj/logs
[[36m2022-10-23 06:26:18,718[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/4[0m
[[36m2022-10-23 06:26:18,718[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:26:18,724[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:26:18,724[0m][[35mHYDRA[0m] 	#5 : trainer.max_epochs=300 model.optimizer.lr=0.0003927308476779979 model.eta=18.880866123061907 model.gamma=0.4070138486494157 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 06:26:18,834[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:26:18,836[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0003927308476779979                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 18.880866123061907                                                 
â”‚       gamma: 0.4070138486494157                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 06:26:18,863[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:26:18,863[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:26:18,864[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:26:18,880[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:26:18,880[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:26:18,882[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:26:18,883[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_062618-3p8l4492
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-wind-71
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3p8l4492
[[36m2022-10-23 06:26:23,432[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:26:23,435[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:26:23,439[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:26:23,439[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:26:23,440[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:26:23,446[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5661/7466 [00:00<00:00, 28508.90it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 33295.52it/s]
[[36m2022-10-23 06:26:23,678[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_2ae298 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_2ae311 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 223.64it/s loss: 181 v_num: 
                                                               4492             
[[36m2022-10-23 06:36:02,908[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:36:02,908[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 42.631 MB of 42.631 MB uploaded (0.000 MB deduped)wandb: \ 42.631 MB of 42.631 MB uploaded (0.000 MB deduped)wandb: | 42.631 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: / 42.631 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: - 42.631 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: \ 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: | 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: / 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: - 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: \ 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: | 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: / 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb: - 42.654 MB of 42.654 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–ˆâ–ƒâ–ˆâ–…â–ƒâ–†â–ƒâ–ˆâ–„â–â–†â–ƒâ–†â–„â–â–„â–â–†â–„â–‡â–„â–â–…â–‚â–‡â–„â–‡â–„â–‚â–…â–‚â–‡â–„â–‡â–…â–‚â–‡
wandb:                 fdr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–ˆâ–ˆâ–ˆâ–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–†â–ˆâ–†â–†â–‡â–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–„â–‚â–‚â–ƒâ–„â–„
wandb:                 tpr â–â–â–â–â–â–ƒâ–†â–†â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–†â–„â–ˆâ–ƒâ–„â–ˆâ–â–‚â–„â–â–‚â–‚â–‚â–ƒâ–†â–â–‚â–ƒâ–‚â–„â–â–â–â–ƒâ–â–â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 1.0
wandb:             lr-Adam 0.00039
wandb:                 shd 22.0
wandb:                 tpr 0.0
wandb:          train/loss 157.81535
wandb: trainer/global_step 96599
wandb: 
wandb: Synced charmed-wind-71: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3p8l4492
wandb: Synced 6 W&B file(s), 1426 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_062618-3p8l4492/logs
[[36m2022-10-23 06:36:08,109[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/5[0m
[[36m2022-10-23 06:36:08,110[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:36:08,114[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:36:08,114[0m][[35mHYDRA[0m] 	#6 : trainer.max_epochs=300 model.optimizer.lr=0.009175498627796746 model.eta=1.6364139758502319 model.gamma=0.46781544701426847 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 06:36:08,223[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:36:08,224[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.009175498627796746                                              
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 1.6364139758502319                                                 
â”‚       gamma: 0.46781544701426847                                              
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 06:36:08,249[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:36:08,249[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:36:08,251[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:36:08,265[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:36:08,266[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:36:08,267[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:36:08,267[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:36:08,267[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:36:08,268[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:36:08,268[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_063608-leq0nids
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-plant-72
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/leq0nids
[[36m2022-10-23 06:36:13,596[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:36:13,599[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:36:13,600[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:36:13,600[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:36:13,601[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:36:13,607[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 7296/7466 [00:00<00:00, 72953.46it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 72768.47it/s]
[[36m2022-10-23 06:36:13,716[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_8a2fc3 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_8a3038 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 229.57it/s loss: 1.38e+05   
                                                               v_num: nids      
[[36m2022-10-23 06:48:17,407[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:48:17,407[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 90.254 MB of 90.254 MB uploaded (0.000 MB deduped)wandb: \ 90.254 MB of 90.254 MB uploaded (0.000 MB deduped)wandb: | 90.254 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: / 90.254 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: - 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: \ 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: | 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: / 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: - 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: \ 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: | 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb: / 90.277 MB of 90.277 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–‡â–…â–‚â–‡â–…â–ƒâ–ˆâ–…â–‚â–â–†â–ƒâ–ˆâ–…â–„â–‚â–†â–„â–â–‡â–…â–‚â–‡â–„â–ƒâ–ˆâ–…â–‚â–‡â–†â–ƒâ–ˆâ–…â–ƒâ–‚â–†â–„â–â–‡
wandb:                 fdr â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–†â–†â–‡â–…â–ˆâ–‡â–‡â–…â–†â–‡â–‡â–ˆâ–ˆâ–†â–†â–†â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–†â–†â–…â–ˆâ–ˆâ–†â–…â–‡â–†â–†â–„â–„â–…â–…â–„â–…â–…â–„â–„â–„â–…â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:                 tpr â–†â–‡â–ˆâ–†â–†â–ˆâ–†â–ˆâ–†â–„â–‡â–‡â–‡â–†â–†â–†â–†â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–â–â–ƒâ–ƒâ–â–‚â–â–â–â–â–â–â–â–
wandb:          train/loss â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–‚â–â–„â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00918
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 165410.32812
wandb: trainer/global_step 113399
wandb: 
wandb: Synced wise-plant-72: https://wandb.ai/flpie/DAG_GCN_optuna/runs/leq0nids
wandb: Synced 6 W&B file(s), 1674 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_063608-leq0nids/logs
[[36m2022-10-23 06:48:22,423[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/6[0m
[[36m2022-10-23 06:48:22,424[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:48:22,428[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:48:22,428[0m][[35mHYDRA[0m] 	#7 : trainer.max_epochs=300 model.optimizer.lr=0.0003902135171882364 model.eta=17.337087295156007 model.gamma=0.9683057858638486 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 06:48:22,542[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:48:22,543[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0003902135171882364                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 17.337087295156007                                                 
â”‚       gamma: 0.9683057858638486                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 06:48:22,570[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:48:22,570[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:48:22,571[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:48:22,587[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:48:22,587[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:48:22,588[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:48:22,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_064822-rnx8ttjd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-eon-73
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/rnx8ttjd
[[36m2022-10-23 06:48:26,321[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:48:26,324[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:48:26,325[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:48:26,325[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:48:26,326[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:48:26,331[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 76517.22it/s]
[[36m2022-10-23 06:48:26,436[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_3fe041 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_3fe0b9 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 215.69it/s loss: 984 v_num: 
                                                               ttjd             
[[36m2022-10-23 06:58:09,741[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 06:58:09,741[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 42.280 MB of 42.280 MB uploaded (0.000 MB deduped)wandb: \ 42.280 MB of 42.280 MB uploaded (0.000 MB deduped)wandb: | 42.280 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: / 42.280 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: - 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: \ 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: | 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: / 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: - 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: \ 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: | 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb: / 42.303 MB of 42.303 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–ˆâ–ƒâ–ˆâ–…â–ƒâ–†â–ƒâ–ˆâ–„â–â–†â–ƒâ–†â–„â–â–„â–â–†â–„â–‡â–„â–â–…â–‚â–‡â–„â–‡â–„â–‚â–…â–‚â–‡â–„â–‡â–…â–‚â–‡
wandb:                 fdr â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–†â–ˆâ–‡â–…â–„â–„â–„â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 tpr â–â–â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ƒâ–‚â–ƒâ–‚â–ˆâ–ƒâ–â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00039
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1030.89514
wandb: trainer/global_step 96599
wandb: 
wandb: Synced wise-eon-73: https://wandb.ai/flpie/DAG_GCN_optuna/runs/rnx8ttjd
wandb: Synced 6 W&B file(s), 1426 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_064822-rnx8ttjd/logs
[[36m2022-10-23 06:58:15,140[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/7[0m
[[36m2022-10-23 06:58:15,140[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 06:58:15,145[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 06:58:15,145[0m][[35mHYDRA[0m] 	#8 : trainer.max_epochs=300 model.optimizer.lr=0.006727248676069746 model.eta=14.9122122598956 model.gamma=0.5981938024256199 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 06:58:15,260[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 06:58:15,261[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.GELU                                               
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.006727248676069746                                              
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 14.9122122598956                                                   
â”‚       gamma: 0.5981938024256199                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 06:58:15,287[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 06:58:15,288[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 06:58:15,289[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 06:58:15,304[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 06:58:15,304[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 06:58:15,306[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 06:58:15,306[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 06:58:15,306[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 06:58:15,307[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 06:58:15,307[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_065815-aoa2vckp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-pond-74
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/aoa2vckp
[[36m2022-10-23 06:58:19,817[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 06:58:19,820[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 06:58:19,821[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 06:58:19,821[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 06:58:19,822[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 06:58:19,831[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 7418/7466 [00:00<00:00, 74173.53it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 73999.17it/s]
[[36m2022-10-23 06:58:19,939[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_a129bc â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_a12a34 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 228.95it/s loss: 3.1e+04    
                                                               v_num: vckp      
[[36m2022-10-23 07:09:04,876[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:09:04,876[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 75.198 MB of 75.198 MB uploaded (0.000 MB deduped)wandb: \ 75.198 MB of 75.198 MB uploaded (0.000 MB deduped)wandb: | 75.198 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: / 75.198 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: - 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: \ 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: | 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: / 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: - 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: \ 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: | 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb: / 75.221 MB of 75.221 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–ƒâ–‡â–…â–‚â–‡â–„â–â–…â–‚â–‡â–„â–â–†â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–„â–â–†â–ƒâ–ˆâ–…â–â–†â–ƒâ–ˆâ–…â–‚â–‡
wandb:                 fdr â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–…â–†â–…â–†â–†â–ˆâ–ˆâ–†â–â–â–â–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–‡â–†â–‡â–‡â–‡â–ˆâ–…â–†â–ˆâ–‡â–†â–†â–†â–„â–…â–„â–„â–‡â–‡â–‡â–‡â–‡â–†â–…â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–
wandb:                 tpr â–‡â–†â–‡â–‡â–ˆâ–…â–ˆâ–†â–…â–„â–…â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–†â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–â–â–
wandb:          train/loss â–‚â–â–ƒâ–ƒâ–‚â–‚â–â–‚â–†â–â–‡â–â–â–â–‚â–ƒâ–‚â–‡â–ˆâ–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–„â–„â–ƒâ–â–â–…â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 1.0
wandb:             lr-Adam 0.00673
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 39855.86719
wandb: trainer/global_step 100799
wandb: 
wandb: Synced distinctive-pond-74: https://wandb.ai/flpie/DAG_GCN_optuna/runs/aoa2vckp
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_065815-aoa2vckp/logs
[[36m2022-10-23 07:09:09,448[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/8[0m
[[36m2022-10-23 07:09:09,448[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:09:09,453[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:09:09,453[0m][[35mHYDRA[0m] 	#9 : trainer.max_epochs=300 model.optimizer.lr=0.0019229650606389275 model.eta=6.941287542379018 model.gamma=0.3269184093556379 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:09:09,567[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:09:09,568[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0019229650606389275                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 6.941287542379018                                                  
â”‚       gamma: 0.3269184093556379                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 07:09:09,595[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:09:09,595[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:09:09,596[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:09:09,612[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:09:09,612[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:09:09,613[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:09:09,613[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:09:09,614[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:09:09,614[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:09:09,614[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_070909-13cmg0ep
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sound-75
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/13cmg0ep
[[36m2022-10-23 07:09:14,648[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:09:14,651[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:09:14,652[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:09:14,652[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:09:14,653[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:09:14,659[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 7362/7466 [00:00<00:00, 73610.94it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 73399.73it/s]
[[36m2022-10-23 07:09:14,767[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_27292c â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_2729a6 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 221.47it/s loss: 1.11e+03   
                                                               v_num: g0ep      
[[36m2022-10-23 07:19:34,397[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:19:34,397[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 50.714 MB of 50.714 MB uploaded (0.000 MB deduped)wandb: \ 50.714 MB of 50.714 MB uploaded (0.000 MB deduped)wandb: | 50.714 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: / 50.714 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: - 50.734 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: \ 50.734 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: | 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: / 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: - 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: \ 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: | 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: / 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb: - 50.737 MB of 50.737 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–ƒâ–‡â–…â–‚â–‡â–„â–â–…â–‚â–‡â–„â–â–†â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–„â–â–†â–ƒâ–ˆâ–…â–â–†â–ƒâ–ˆâ–…â–‚â–‡
wandb:                 fdr â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–†â–†â–…â–†â–‡â–ˆâ–…â–‡â–‡â–…â–†â–…â–…â–…â–…â–…â–‡â–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 tpr â–ƒâ–…â–†â–†â–†â–†â–ˆâ–…â–ˆâ–…â–…â–…â–…â–â–â–…â–â–â–â–â–ƒâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–‚â–â–â–â–â–â–â–â–ƒâ–â–ˆâ–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–‚â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00192
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1021.60461
wandb: trainer/global_step 100799
wandb: 
wandb: Synced royal-sound-75: https://wandb.ai/flpie/DAG_GCN_optuna/runs/13cmg0ep
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_070909-13cmg0ep/logs
[[36m2022-10-23 07:19:38,794[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/9[0m
[[36m2022-10-23 07:19:38,794[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:19:38,798[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:19:38,799[0m][[35mHYDRA[0m] 	#10 : trainer.max_epochs=300 model.optimizer.lr=0.007514548626225465 model.eta=6.4873111276534186 model.gamma=0.6207304740806995 model.activation=torch.nn.GELU[0m
[[36m2022-10-23 07:19:38,911[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:19:38,912[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.GELU                                               
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.GELU                                             
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.007514548626225465                                              
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 6.4873111276534186                                                 
â”‚       gamma: 0.6207304740806995                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 07:19:38,939[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:19:38,939[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:19:38,940[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:19:38,956[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:19:38,956[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:19:38,957[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:19:38,958[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_071938-2luc429r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-thunder-76
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2luc429r
[[36m2022-10-23 07:19:43,684[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:19:43,688[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:19:43,688[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:19:43,689[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:19:43,695[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 7457/7466 [00:00<00:00, 74563.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 74359.57it/s]
[[36m2022-10-23 07:19:43,802[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_9e476b â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_9e47e7 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ GELU              â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ GELU              â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 231.84it/s loss: 6.75e+04   
                                                               v_num: 429r      
[[36m2022-10-23 07:30:56,037[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:30:56,037[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 78.856 MB of 78.856 MB uploaded (0.000 MB deduped)wandb: \ 78.856 MB of 78.856 MB uploaded (0.000 MB deduped)wandb: | 78.856 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: / 78.856 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: - 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: \ 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: | 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: / 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: - 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: \ 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: | 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb: / 78.879 MB of 78.879 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆ
wandb:                 fdr â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–†â–†â–…â–‡â–†â–†â–‡â–ˆâ–‡â–†â–‡â–‡â–‡â–‡â–…â–†â–…â–…â–â–â–â–…â–…â–†â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–…â–†â–ˆâ–…â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–„â–†â–ˆâ–†â–„â–‚â–…â–…â–„â–…â–†â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–‚
wandb:                 tpr â–…â–†â–‡â–‡â–†â–†â–†â–ˆâ–†â–‡â–†â–…â–…â–†â–…â–…â–ƒâ–…â–…â–‚â–â–‚â–ƒâ–â–â–‚â–â–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–
wandb:          train/loss â–‚â–â–…â–‚â–â–â–â–‚â–â–ˆâ–ƒâ–‚â–‚â–†â–â–â–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–ƒâ–â–â–â–â–ƒâ–‚â–„â–‚â–‚â–‚
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 1.0
wandb:             lr-Adam 0.00751
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 90967.5625
wandb: trainer/global_step 104999
wandb: 
wandb: Synced devoted-thunder-76: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2luc429r
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_071938-2luc429r/logs
[[36m2022-10-23 07:31:01,255[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/10[0m
[[36m2022-10-23 07:31:01,255[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:31:01,260[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:31:01,260[0m][[35mHYDRA[0m] 	#11 : trainer.max_epochs=300 model.optimizer.lr=0.002796126334085001 model.eta=4.909994085152537 model.gamma=0.8365797926796967 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:31:01,372[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:31:01,373[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.002796126334085001                                              
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 4.909994085152537                                                  
â”‚       gamma: 0.8365797926796967                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 07:31:01,400[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:31:01,400[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:31:01,401[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:31:01,529[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:31:01,529[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:31:01,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:31:01,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:31:01,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:31:01,532[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:31:01,532[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_073101-2shqld3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-spaceship-77
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2shqld3l
[[36m2022-10-23 07:31:06,014[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:31:06,019[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:31:06,020[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:31:06,020[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:31:06,021[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:31:06,026[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 75283.62it/s]
[[36m2022-10-23 07:31:06,133[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_351fe1 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_35205d â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 230.19it/s loss: 1.47e+03   
                                                               v_num: ld3l      
[[36m2022-10-23 07:42:00,391[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:42:00,391[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 62.783 MB of 62.783 MB uploaded (0.000 MB deduped)wandb: \ 62.783 MB of 62.783 MB uploaded (0.000 MB deduped)wandb: | 62.783 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: / 62.786 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: - 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: \ 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: | 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: / 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: - 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: \ 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb: | 62.806 MB of 62.806 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆ
wandb:                 fdr â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ƒâ–†â–‡â–‡â–†â–†â–†â–â–ˆâ–â–â–ˆâ–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–„â–…â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–†â–†â–…â–…â–ƒâ–„â–â–‚â–ƒâ–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 tpr â–„â–ƒâ–…â–…â–‡â–„â–„â–†â–ˆâ–‡â–†â–…â–…â–†â–†â–†â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–„â–â–‚â–â–â–â–â–‚â–‚â–†â–â–‚â–‚â–ˆâ–‚â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.0028
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1993.48792
wandb: trainer/global_step 104999
wandb: 
wandb: Synced volcanic-spaceship-77: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2shqld3l
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_073101-2shqld3l/logs
[[36m2022-10-23 07:42:05,412[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/11[0m
[[36m2022-10-23 07:42:05,412[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:42:05,417[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:42:05,417[0m][[35mHYDRA[0m] 	#12 : trainer.max_epochs=300 model.optimizer.lr=0.0017053336568535853 model.eta=8.370741811450799 model.gamma=0.7805833696113673 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:42:05,531[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:42:05,532[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0017053336568535853                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 8.370741811450799                                                  
â”‚       gamma: 0.7805833696113673                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 07:42:05,558[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:42:05,559[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:42:05,560[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:42:05,576[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:42:05,576[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:42:05,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:42:05,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:42:05,578[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:42:05,578[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:42:05,578[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_074205-ycvq9d7d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run breezy-haze-78
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/ycvq9d7d
[[36m2022-10-23 07:42:10,192[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:42:10,195[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:42:10,196[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:42:10,196[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:42:10,197[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:42:10,203[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 5661/7466 [00:00<00:00, 30108.08it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 34922.48it/s]
[[36m2022-10-23 07:42:10,424[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_c0ed47 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_c0edc0 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 224.87it/s loss: 168 v_num: 
                                                               9d7d             
[[36m2022-10-23 07:53:16,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 07:53:16,577[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 54.411 MB of 54.425 MB uploaded (0.000 MB deduped)wandb: \ 54.411 MB of 54.425 MB uploaded (0.000 MB deduped)wandb: | 54.411 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.434 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: \ 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: | 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: / 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb: - 54.448 MB of 54.448 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–‡â–„â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–ƒâ–ˆâ–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–‡
wandb:                 fdr â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–„â–„â–„â–„â–†â–‡â–‡â–†â–…â–†â–„â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 tpr â–ƒâ–…â–†â–ˆâ–†â–…â–…â–ƒâ–ƒâ–†â–…â–ƒâ–ƒâ–ƒâ–…â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–‚â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–â–ƒâ–â–â–â–â–ˆâ–â–â–â–…â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00171
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 198.38333
wandb: trainer/global_step 109199
wandb: 
wandb: Synced breezy-haze-78: https://wandb.ai/flpie/DAG_GCN_optuna/runs/ycvq9d7d
wandb: Synced 6 W&B file(s), 1612 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_074205-ycvq9d7d/logs
[[36m2022-10-23 07:54:54,690[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/12[0m
[[36m2022-10-23 07:54:54,690[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 07:54:54,695[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 07:54:54,695[0m][[35mHYDRA[0m] 	#13 : trainer.max_epochs=300 model.optimizer.lr=0.00467193103159237 model.eta=2.336688963322965 model.gamma=0.9976062012462577 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 07:54:54,806[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 07:54:54,807[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.00467193103159237                                               
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 2.336688963322965                                                  
â”‚       gamma: 0.9976062012462577                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 07:54:54,834[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 07:54:54,834[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 07:54:54,835[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 07:54:54,851[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 07:54:54,851[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 07:54:54,852[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 07:54:54,853[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_075454-2x8g4ore
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-fog-79
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2x8g4ore
[[36m2022-10-23 07:54:59,353[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 07:54:59,358[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 07:54:59,359[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 07:54:59,359[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 07:54:59,360[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 07:54:59,367[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7233/7466 [00:00<00:00, 72327.31it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 72110.59it/s]
[[36m2022-10-23 07:54:59,484[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_8b7355 â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_8b73cd â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 218.64it/s loss: 2.45e+03   
                                                               v_num: 4ore      
[[36m2022-10-23 08:05:40,498[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 08:05:40,498[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 69.083 MB of 69.083 MB uploaded (0.000 MB deduped)wandb: \ 69.083 MB of 69.083 MB uploaded (0.000 MB deduped)wandb: | 69.083 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: / 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: - 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: \ 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: | 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: / 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: - 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: \ 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: | 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb: / 69.106 MB of 69.106 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–ƒâ–‡â–…â–‚â–‡â–„â–â–…â–‚â–‡â–„â–â–†â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–„â–â–†â–ƒâ–ˆâ–…â–â–†â–ƒâ–ˆâ–…â–‚â–‡
wandb:                 fdr â–‡â–‡â–†â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–…â–†â–‡â–†â–‡â–…â–‡â–†â–†â–†â–„â–…â–…â–…â–…â–‡â–ˆâ–â–â–â–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–…â–„â–†â–‡â–†â–ˆâ–†â–‡â–ˆâ–†â–†â–‡â–‡â–†â–ˆâ–†â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–„â–‚â–ƒâ–‚â–â–â–â–â–â–ƒâ–‚â–â–â–â–‚
wandb:                 tpr â–‚â–„â–‡â–…â–†â–…â–ˆâ–†â–„â–„â–„â–…â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–
wandb:          train/loss â–â–â–‚â–‚â–â–â–â–‚â–…â–â–ˆâ–‚â–â–‚â–‚â–ƒâ–‚â–‚â–…â–‚â–‚â–â–ƒâ–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–ƒâ–â–â–â–â–‚
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00467
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 1848.41663
wandb: trainer/global_step 100799
wandb: 
wandb: Synced sunny-fog-79: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2x8g4ore
wandb: Synced 6 W&B file(s), 1488 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_075454-2x8g4ore/logs
[[36m2022-10-23 08:05:45,268[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/13[0m
[[36m2022-10-23 08:05:45,268[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 08:05:45,273[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-23 08:05:45,273[0m][[35mHYDRA[0m] 	#14 : trainer.max_epochs=300 model.optimizer.lr=0.0021503938144996396 model.eta=13.500535599345383 model.gamma=0.2815666977596547 model.activation=torch.nn.LeakyReLU[0m
[[36m2022-10-23 08:05:45,537[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-23 08:05:45,538[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: false                                                       
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: false                                                     
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.0021503938144996396                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 13.500535599345383                                                 
â”‚       gamma: 0.2815666977596547                                               
â”‚       graph_threshold: 0.3                                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ k_max_iter
â”‚   â””â”€â”€ 10                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 300                                                         
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-23 08:05:45,566[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-23 08:05:45,566[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-23 08:05:45,567[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-23 08:05:45,583[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-23 08:05:45,583[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-23 08:05:45,584[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-23 08:05:45,585[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221023_080545-25z09gb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-butterfly-80
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/25z09gb6
[[36m2022-10-23 08:05:49,980[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-23 08:05:49,983[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-23 08:05:49,983[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-23 08:05:49,983[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-23 08:05:49,984[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-23 08:05:49,984[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-23 08:05:49,984[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-23 08:05:49,985[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-23 08:05:49,994[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 77235.70it/s]
[[36m2022-10-23 08:05:50,098[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                       â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                    â”‚ DAG_GCN_Encoder   â”‚  8.6 K â”‚
â”‚ 1  â”‚ encoder.model              â”‚ Sequential_0f512d â”‚  8.5 K â”‚
â”‚ 2  â”‚ encoder.model.module_0     â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 4  â”‚ encoder.model.module_2     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 5  â”‚ encoder.model.module_3     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 6  â”‚ encoder.model.module_3.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 7  â”‚ encoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 8  â”‚ encoder.model.module_5     â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 9  â”‚ encoder.model.module_5.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 10 â”‚ decoder                    â”‚ DAG_GCN_Decoder   â”‚  8.5 K â”‚
â”‚ 11 â”‚ decoder.model              â”‚ Sequential_0f51a8 â”‚  8.5 K â”‚
â”‚ 12 â”‚ decoder.model.module_0     â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 13 â”‚ decoder.model.module_0.lin â”‚ Linear            â”‚     64 â”‚
â”‚ 14 â”‚ decoder.model.module_1     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ decoder.model.module_2     â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 16 â”‚ decoder.model.module_2.lin â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 17 â”‚ decoder.model.module_3     â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 18 â”‚ decoder.model.module_4     â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 19 â”‚ decoder.model.module_5     â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.1 K                                                        
Non-trainable params: 0                                                         
Total params: 17.1 K                                                            
Total estimated model params size (MB): 0                                       
Epoch 299 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 220.29it/s loss: 233 v_num: 
                                                               9gb6             
[[36m2022-10-23 08:16:40,691[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-23 08:16:40,691[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 57.122 MB of 57.122 MB uploaded (0.000 MB deduped)wandb: \ 57.122 MB of 57.122 MB uploaded (0.000 MB deduped)wandb: | 57.122 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: / 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: - 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: \ 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: | 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: / 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: - 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: \ 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb: | 57.145 MB of 57.145 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–‚â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆâ–…â–‚â–‡â–…â–‚â–‡â–„â–â–†â–ƒâ–ˆâ–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–â–†â–ƒâ–ˆ
wandb:                 fdr â–ˆâ–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–â–ˆâ–ˆâ–ˆâ–â–â–â–â–â–
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–…â–ƒâ–ƒâ–‚â–„â–…â–…â–…â–†â–ˆâ–‡â–†â–„â–„â–ƒâ–„â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:                 tpr â–‚â–…â–…â–ˆâ–†â–‡â–…â–…â–†â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–‚â–â–‚â–â–â–‚â–â–„â–â–‡â–‚â–â–‚â–ˆâ–‚â–â–‚â–â–ƒâ–â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–
wandb: trainer/global_step â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 299
wandb:                 fdr 0.0
wandb:             lr-Adam 0.00215
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 317.24094
wandb: trainer/global_step 104999
wandb: 
wandb: Synced winter-butterfly-80: https://wandb.ai/flpie/DAG_GCN_optuna/runs/25z09gb6
wandb: Synced 6 W&B file(s), 1550 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221023_080545-25z09gb6/logs
[[36m2022-10-23 08:16:45,703[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-23_05-31-43/14[0m
[[36m2022-10-23 08:16:45,703[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-23 08:16:45,704[0m][[35mHYDRA[0m] Best parameters: {'trainer.max_epochs': 300, 'model.optimizer.lr': 0.0019960425587510337, 'model.eta': 12.820066649756805, 'model.gamma': 0.493954965106403, 'model.activation': 'torch.nn.LeakyReLU'}[0m
[[36m2022-10-23 08:16:45,704[0m][[35mHYDRA[0m] Best value: 0.0[0m
[32m[I 2022-10-24 03:57:27,780][0m A new study created in memory with name: no-name-5697e595-cda4-44c1-b447-5b7d743702a4[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Study name: no-name-5697e595-cda4-44c1-b447-5b7d743702a4[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2022-10-24 03:57:27,780[0m][[35mHYDRA[0m] Directions: ['maximize'][0m
[[36m2022-10-24 03:57:27,783[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 03:57:27,783[0m][[35mHYDRA[0m] 	#0 : model.graph_threshold=0.17660778015155693 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 03:57:28,663[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 03:57:28,667[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.17660778015155693                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 03:57:28,782[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 03:57:28,782[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 03:57:28,916[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
[[36m2022-10-24 03:57:29,026[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpe4mei9q6[0m
[[36m2022-10-24 03:57:29,026[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpe4mei9q6/_remote_module_non_scriptable.py[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 03:57:29,044[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 03:57:29,044[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 03:57:29,046[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 03:57:29,046[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 03:57:29,047[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 03:57:29,047[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 03:57:29,047[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: flpie. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_035730-2p899k9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-pine-106
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2p899k9h
[[36m2022-10-24 03:57:33,945[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 03:57:33,949[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 03:57:33,949[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 03:57:33,950[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 03:57:33,957[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 6338/7466 [00:00<00:00, 63372.05it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 63341.30it/s]
[[36m2022-10-24 03:57:34,769[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_8ab0cf â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_8ab167 â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('train/accu', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('shd', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/acc')` could not find the monitored key in the returned metrics: ['train/loss', 'train/losses', 'train/accu', 'shd', 'tpr', 'fdr', 'epoch', 'step']. HINT: Did you call `log('val/acc', value)` in the `LightningModule`?
  warning_cache.warn(m)
Exception in thread MsgRouterThr:
Traceback (most recent call last):
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 69, in message_loop
    msg = self._read_message()
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py", line 32, in _read_message
    msg = self._response_queue.get(timeout=1)
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/queues.py", line 117, in get
    res = self._recv_bytes()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 217, in recv_bytes
    self._check_closed()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 141, in _check_closed
    raise OSError("handle is closed")
OSError: handle is closed
[32m[I 2022-10-24 03:59:34,817][0m A new study created in memory with name: no-name-453d9238-e7d0-4c01-96a5-a93d6df979a2[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Study name: no-name-453d9238-e7d0-4c01-96a5-a93d6df979a2[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Storage: None[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Sampler: TPESampler[0m
[[36m2022-10-24 03:59:34,817[0m][[35mHYDRA[0m] Directions: ['maximize'][0m
[[36m2022-10-24 03:59:34,819[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 03:59:34,819[0m][[35mHYDRA[0m] 	#0 : model.graph_threshold=0.17660778015155693 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 03:59:35,684[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 03:59:35,689[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.17660778015155693                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 03:59:35,799[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 03:59:35,800[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 03:59:35,934[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
[[36m2022-10-24 03:59:36,045[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Created a temporary directory at /tmp/tmpug6gabfw[0m
[[36m2022-10-24 03:59:36,045[0m][[34mtorch.distributed.nn.jit.instantiator[0m][[32mINFO[0m] - Writing /tmp/tmpug6gabfw/_remote_module_non_scriptable.py[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 03:59:36,063[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 03:59:36,064[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 03:59:36,066[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 03:59:36,066[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 03:59:36,066[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 03:59:36,067[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 03:59:36,067[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: Currently logged in as: flpie. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_035937-3dyl6otg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-dawn-107
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3dyl6otg
[[36m2022-10-24 03:59:40,862[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 03:59:40,866[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 03:59:40,867[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 03:59:40,867[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 03:59:40,868[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 03:59:40,875[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 6538/7466 [00:00<00:00, 65182.85it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 64763.70it/s]
[[36m2022-10-24 03:59:41,685[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_d66668 â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_d66702 â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('train/accu', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('shd', ...)` in your `training_epoch_end` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:378: UserWarning: `ModelCheckpoint(monitor='val/acc')` could not find the monitored key in the returned metrics: ['train/loss', 'train/losses', 'train/accu', 'shd', 'tpr', 'fdr', 'epoch', 'step']. HINT: Did you call `log('val/acc', value)` in the `LightningModule`?
  warning_cache.warn(m)
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 188.69it/s loss: 7.83e+03   
                                                               v_num: 6otg      
[[36m2022-10-24 04:41:39,840[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 04:41:39,840[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 241.541 MB of 241.541 MB uploaded (0.000 MB deduped)wandb: \ 241.541 MB of 241.541 MB uploaded (0.000 MB deduped)wandb: | 241.541 MB of 241.541 MB uploaded (0.000 MB deduped)wandb: / 241.541 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: - 241.541 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: \ 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: | 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: / 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: - 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: \ 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: | 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: / 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: - 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb: \ 241.567 MB of 241.567 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–â–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–…â–„â–ƒâ–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–ˆâ–‡â–†â–…â–…â–„â–„â–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–
wandb:                 tpr â–ˆâ–ˆâ–‡â–†â–…â–…â–…â–†â–…â–…â–†â–†â–…â–†â–†â–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 1.0
wandb:             lr-Adam 0.001
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced wandering-dawn-107: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3dyl6otg
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_035937-3dyl6otg/logs
[[36m2022-10-24 04:41:45,581[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/0[0m
[[36m2022-10-24 04:41:45,581[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 04:41:45,584[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 04:41:45,584[0m][[35mHYDRA[0m] 	#1 : model.graph_threshold=0.34884350841593276 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 04:41:45,698[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 04:41:45,699[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.34884350841593276                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 04:41:45,728[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 04:41:45,728[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 04:41:45,729[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 04:41:45,749[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 04:41:45,749[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 04:41:45,751[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 04:41:45,752[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_044145-2l7tie1o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-sky-108
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2l7tie1o
[[36m2022-10-24 04:41:50,431[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 04:41:50,434[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 04:41:50,434[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 04:41:50,434[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 04:41:50,435[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 04:41:50,435[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 04:41:50,435[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 04:41:50,436[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 04:41:50,443[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 7176/7466 [00:00<00:00, 71752.54it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 71706.70it/s]
[[36m2022-10-24 04:41:50,555[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_ba380e â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_ba389f â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 179.33it/s loss: 7.83e+03   
                                                               v_num: ie1o      
[[36m2022-10-24 05:24:10,464[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 05:24:10,464[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 189.695 MB of 189.695 MB uploaded (0.000 MB deduped)wandb: \ 189.695 MB of 189.695 MB uploaded (0.000 MB deduped)wandb: | 189.695 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: / 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: - 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: \ 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: | 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: / 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: - 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: \ 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb: | 189.719 MB of 189.719 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–‡â–‡â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–
wandb:                 tpr â–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced neat-sky-108: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2l7tie1o
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_044145-2l7tie1o/logs
[[36m2022-10-24 05:24:15,048[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/1[0m
[[36m2022-10-24 05:24:15,048[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 05:24:15,051[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 05:24:15,051[0m][[35mHYDRA[0m] 	#2 : model.graph_threshold=0.27509109560284584 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 05:24:15,166[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 05:24:15,167[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.27509109560284584                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 05:24:15,197[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 05:24:15,197[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 05:24:15,198[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 05:24:15,218[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 05:24:15,218[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 05:24:15,220[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 05:24:15,220[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 05:24:15,220[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 05:24:15,221[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 05:24:15,221[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_052415-4tluxkzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-eon-109
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/4tluxkzv
[[36m2022-10-24 05:24:19,835[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 05:24:19,838[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 05:24:19,838[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 05:24:19,839[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 05:24:19,846[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 78265.16it/s]
[[36m2022-10-24 05:24:19,950[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_a9d225 â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_a9d2b3 â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 178.64it/s loss: 7.83e+03   
                                                               v_num: xkzv      
[[36m2022-10-24 06:07:32,076[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 06:07:32,077[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 205.941 MB of 205.941 MB uploaded (0.000 MB deduped)wandb: \ 205.941 MB of 205.941 MB uploaded (0.000 MB deduped)wandb: | 205.941 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: / 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: - 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: \ 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: | 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: / 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: - 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb: \ 205.966 MB of 205.966 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–‡â–‡â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–
wandb:                 tpr â–ˆâ–‡â–†â–…â–…â–ƒâ–„â–…â–…â–…â–…â–„â–…â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced glamorous-eon-109: https://wandb.ai/flpie/DAG_GCN_optuna/runs/4tluxkzv
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_052415-4tluxkzv/logs
[[36m2022-10-24 06:07:37,900[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/2[0m
[[36m2022-10-24 06:07:37,900[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 06:07:37,903[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 06:07:37,903[0m][[35mHYDRA[0m] 	#3 : model.graph_threshold=0.41414343348550775 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 06:07:38,020[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 06:07:38,021[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.41414343348550775                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 06:07:38,049[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 06:07:38,049[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 06:07:38,051[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 06:07:38,071[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 06:07:38,071[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 06:07:38,072[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 06:07:38,073[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 06:07:38,073[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 06:07:38,073[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 06:07:38,074[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_060738-l5xmhom6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-shape-110
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/l5xmhom6
[[36m2022-10-24 06:07:42,681[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 06:07:42,684[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 06:07:42,685[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 06:07:42,685[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 06:07:42,686[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 06:07:42,693[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 77889.06it/s]
[[36m2022-10-24 06:07:42,797[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_b93de1 â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_b93e7d â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 170.20it/s loss: 7.83e+03   
                                                               v_num: hom6      
[[36m2022-10-24 06:50:30,710[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 06:50:30,710[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 180.493 MB of 180.493 MB uploaded (0.000 MB deduped)wandb: \ 180.493 MB of 180.493 MB uploaded (0.000 MB deduped)wandb: | 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: / 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: - 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: \ 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: | 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: / 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: - 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: \ 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb: | 180.518 MB of 180.518 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–ˆâ–‡â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:                 tpr â–ˆâ–‡â–‡â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced peachy-shape-110: https://wandb.ai/flpie/DAG_GCN_optuna/runs/l5xmhom6
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_060738-l5xmhom6/logs
[[36m2022-10-24 06:50:35,294[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/3[0m
[[36m2022-10-24 06:50:35,294[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 06:50:35,299[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 06:50:35,299[0m][[35mHYDRA[0m] 	#4 : model.graph_threshold=0.4119903232475214 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 06:50:35,413[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 06:50:35,414[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.4119903232475214                                     
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 06:50:35,443[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 06:50:35,443[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 06:50:35,444[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 06:50:35,463[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 06:50:35,463[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 06:50:35,465[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 06:50:35,465[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 06:50:35,466[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 06:50:35,466[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 06:50:35,466[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_065035-2atk5yc7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-rain-111
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/2atk5yc7
[[36m2022-10-24 06:50:40,133[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 06:50:40,136[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 06:50:40,136[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 06:50:40,137[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 06:50:40,137[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 06:50:40,137[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 06:50:40,137[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 06:50:40,138[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 06:50:40,143[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 7203/7466 [00:00<00:00, 72025.95it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 72156.95it/s]
[[36m2022-10-24 06:50:40,255[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_b97ccf â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_b97d57 â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 183.34it/s loss: 7.83e+03   
                                                               v_num: 5yc7      
[[36m2022-10-24 07:33:47,199[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 07:33:47,199[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 180.701 MB of 180.701 MB uploaded (0.000 MB deduped)wandb: \ 180.701 MB of 180.701 MB uploaded (0.000 MB deduped)wandb: | 180.701 MB of 180.701 MB uploaded (0.000 MB deduped)wandb: / 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: - 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: \ 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: | 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: / 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: - 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: \ 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb: | 180.725 MB of 180.725 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–ˆâ–‡â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:                 tpr â–ˆâ–‡â–‡â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced fallen-rain-111: https://wandb.ai/flpie/DAG_GCN_optuna/runs/2atk5yc7
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_065035-2atk5yc7/logs
[[36m2022-10-24 07:33:51,955[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/4[0m
[[36m2022-10-24 07:33:51,955[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 07:33:51,959[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 07:33:51,959[0m][[35mHYDRA[0m] 	#5 : model.graph_threshold=0.11182750899709085 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 07:33:52,071[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 07:33:52,072[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.11182750899709085                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 07:33:52,100[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 07:33:52,100[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 07:33:52,101[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 07:33:52,121[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 07:33:52,121[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 07:33:52,123[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 07:33:52,123[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 07:33:52,123[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 07:33:52,124[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 07:33:52,124[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_073352-d47ykhg1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-fire-112
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/d47ykhg1
[[36m2022-10-24 07:33:56,695[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 07:33:56,699[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 07:33:56,699[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 07:33:56,700[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 07:33:56,706[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 75736.81it/s]
[[36m2022-10-24 07:33:56,813[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_c53752 â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_c537e2 â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 177.36it/s loss: 7.83e+03   
                                                               v_num: khg1      
[[36m2022-10-24 08:18:46,359[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 08:18:46,359[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 282.346 MB of 282.346 MB uploaded (0.000 MB deduped)wandb: \ 282.346 MB of 282.346 MB uploaded (0.000 MB deduped)wandb: | 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: / 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: - 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: \ 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: | 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb: / 282.371 MB of 282.371 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–„â–„â–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–„â–„â–ƒâ–…â–ƒâ–„â–ƒâ–â–ƒâ–„â–ƒâ–†â–‚â–ƒâ–†â–ˆâ–ˆâ–…
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–‡â–‡â–†â–…â–…â–„â–…â–„â–„â–…â–„â–…â–…â–ƒâ–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚â–â–
wandb:                 tpr â–ˆâ–‡â–‡â–…â–†â–‡â–†â–†â–†â–†â–†â–†â–†â–†â–…â–†â–†â–†â–†â–…â–†â–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 1.0
wandb:             lr-Adam 0.001
wandb:                 shd 23.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced absurd-fire-112: https://wandb.ai/flpie/DAG_GCN_optuna/runs/d47ykhg1
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_073352-d47ykhg1/logs
[[36m2022-10-24 08:18:51,419[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/5[0m
[[36m2022-10-24 08:18:51,420[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 08:18:51,423[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 08:18:51,423[0m][[35mHYDRA[0m] 	#6 : model.graph_threshold=0.16292055445901682 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 08:18:51,537[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 08:18:51,538[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.16292055445901682                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 08:18:51,566[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 08:18:51,567[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 08:18:51,568[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 08:18:51,587[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 08:18:51,587[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 08:18:51,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 08:18:51,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 08:18:51,589[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 08:18:51,590[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 08:18:51,590[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_081851-3f7pmq2v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sunset-113
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/3f7pmq2v
[[36m2022-10-24 08:18:56,151[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 08:18:56,155[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 08:18:56,156[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 08:18:56,156[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 08:18:56,162[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 80155.72it/s]
[[36m2022-10-24 08:18:56,264[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_0e3947 â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_0e39d9 â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 165.46it/s loss: 7.83e+03   
                                                               v_num: mq2v      
[[36m2022-10-24 09:03:37,622[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 09:03:37,622[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 249.385 MB of 249.385 MB uploaded (0.000 MB deduped)wandb: \ 249.385 MB of 249.385 MB uploaded (0.000 MB deduped)wandb: | 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: / 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: - 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: \ 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: | 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb: / 249.409 MB of 249.409 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–„â–„â–„â–ƒâ–…â–…â–ƒâ–ƒâ–‚â–†â–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–ˆâ–‡â–†â–…â–„â–„â–„â–„â–„â–ƒâ–„â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–‚â–â–
wandb:                 tpr â–ˆâ–ˆâ–‡â–†â–…â–…â–…â–†â–…â–…â–†â–†â–…â–†â–†â–†â–…â–…â–„â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–â–â–â–â–‚â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 1.0
wandb:             lr-Adam 0.001
wandb:                 shd 19.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced wild-sunset-113: https://wandb.ai/flpie/DAG_GCN_optuna/runs/3f7pmq2v
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_081851-3f7pmq2v/logs
[[36m2022-10-24 09:03:41,986[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/6[0m
[[36m2022-10-24 09:03:41,986[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 09:03:41,990[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 09:03:41,990[0m][[35mHYDRA[0m] 	#7 : model.graph_threshold=0.22958692509211015 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 09:03:42,101[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 09:03:42,102[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.22958692509211015                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 09:03:42,129[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 09:03:42,129[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 09:03:42,130[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 09:03:42,149[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 09:03:42,150[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 09:03:42,151[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 09:03:42,152[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_090342-30jmio8r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stellar-rain-114
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/30jmio8r
[[36m2022-10-24 09:03:46,668[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 09:03:46,671[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 09:03:46,671[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 09:03:46,672[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 09:03:46,672[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 09:03:46,672[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 09:03:46,672[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 09:03:46,673[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 09:03:46,679[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 7338/7466 [00:00<00:00, 73369.75it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 73264.92it/s]
[[36m2022-10-24 09:03:46,789[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_51ec7d â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_51ed0b â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 999 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 14/14 0:00:00 â€¢ 0:00:00 182.31it/s loss: 7.83e+03   
                                                               v_num: io8r      
[[36m2022-10-24 09:47:55,041[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing loggers...[0m
[[36m2022-10-24 09:47:55,041[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Closing wandb![0m
wandb: Waiting for W&B process to finish... (success).
wandb: - 218.282 MB of 218.282 MB uploaded (0.000 MB deduped)wandb: \ 218.282 MB of 218.282 MB uploaded (0.000 MB deduped)wandb: | 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: / 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: - 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: \ 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: | 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb: / 218.307 MB of 218.307 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch â–ƒâ–†â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–‡â–„â–‚â–†â–„â–ˆâ–„â–ƒâ–†â–…â–â–‡â–ƒâ–â–…â–ƒâ–‡â–…â–‚â–†â–„â–ˆâ–†â–‚â–ˆâ–„â–ƒâ–†â–…â–â–‡
wandb:                 fdr â–…â–„â–…â–…â–„â–„â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–†â–†â–…â–…â–„â–„â–ƒâ–…â–„â–‚â–†â–„â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:             lr-Adam â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 shd â–ˆâ–‡â–‡â–†â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–‚â–‚â–â–
wandb:                 tpr â–ˆâ–ˆâ–‡â–…â–…â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–
wandb:          train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: trainer/global_step â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:               epoch 999
wandb:                 fdr 0.0
wandb:             lr-Adam 0.001
wandb:                 shd 18.0
wandb:                 tpr 0.0
wandb:          train/loss 8406.09668
wandb: trainer/global_step 349999
wandb: 
wandb: Synced stellar-rain-114: https://wandb.ai/flpie/DAG_GCN_optuna/runs/30jmio8r
wandb: Synced 6 W&B file(s), 5050 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221024_090342-30jmio8r/logs
[[36m2022-10-24 09:47:59,774[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Output dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/DAG_GNN_lightning/logs/train/multiruns/2022-10-24_03-59-33/7[0m
[[36m2022-10-24 09:47:59,774[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Retrieved metric value! <tpr=0.0>[0m
[[36m2022-10-24 09:47:59,778[0m][[35mHYDRA[0m] Launching 1 jobs locally[0m
[[36m2022-10-24 09:47:59,778[0m][[35mHYDRA[0m] 	#8 : model.graph_threshold=0.20962010369362757 hparams_search=DAG_GCN_optuna[0m
[[36m2022-10-24 09:47:59,891[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Enforcing tags! <cfg.extras.enforce_tags=True>[0m
[[36m2022-10-24 09:47:59,892[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Printing config tree with Rich! <cfg.extras.print_config=True>[0m
CONFIG
â”œâ”€â”€ datamodule
â”‚   â””â”€â”€ _target_: src.datamodules.sachs_datamodule.SachsDataModule              
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       batch_size: 512                                                         
â”‚       shuffle: true                                                           
â”‚                                                                               
â”œâ”€â”€ model
â”‚   â””â”€â”€ _target_: src.models.DAG_GCN.DAG_GCN                                    
â”‚       activation: torch.nn.LeakyReLU                                          
â”‚       batch_norm: true                                                        
â”‚       init: true                                                              
â”‚       encoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Encoder    
â”‚         in_channels: 1                                                        
â”‚         H1: 64                                                                
â”‚         H2: 64                                                                
â”‚         H3: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       decoder:                                                                
â”‚         _target_: src.models.components.DAG_GCN_components.DAG_GCN_Decoder    
â”‚         in_channels: 1                                                        
â”‚         H3: 64                                                                
â”‚         H2: 64                                                                
â”‚         H1: 64                                                                
â”‚         out_channels: 1                                                       
â”‚         activation:                                                           
â”‚           _target_: torch.nn.LeakyReLU                                        
â”‚         batch_norm: true                                                      
â”‚         num_features: 11                                                      
â”‚       optimizer:                                                              
â”‚         _target_: torch.optim.Adam                                            
â”‚         _partial_: true                                                       
â”‚         lr: 0.001                                                             
â”‚         weight_decay: 0.0                                                     
â”‚       num_features: 11                                                        
â”‚       batch_size: 512                                                         
â”‚       lambda_A: 0.0                                                           
â”‚       c_A: 1                                                                  
â”‚       eta: 10                                                                 
â”‚       gamma: 0.25                                                             
â”‚       graph_threshold: 0.20962010369362757                                    
â”‚       adj_high: 0.1                                                           
â”‚       adj_low: -0.1                                                           
â”‚       plot_every: 10                                                          
â”‚       gt_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       raw_path: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ callbacks
â”‚   â””â”€â”€ model_checkpoint:                                                       
â”‚         _target_: pytorch_lightning.callbacks.ModelCheckpoint                 
â”‚         dirpath: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspac
â”‚         filename: epoch_{epoch:03d}                                           
â”‚         monitor: val/acc                                                      
â”‚         verbose: false                                                        
â”‚         save_last: true                                                       
â”‚         save_top_k: 1                                                         
â”‚         mode: max                                                             
â”‚         auto_insert_metric_name: false                                        
â”‚         save_weights_only: false                                              
â”‚         every_n_train_steps: null                                             
â”‚         train_time_interval: null                                             
â”‚         every_n_epochs: null                                                  
â”‚         save_on_train_epoch_end: null                                         
â”‚       early_stopping: false                                                   
â”‚       model_summary:                                                          
â”‚         _target_: pytorch_lightning.callbacks.RichModelSummary                
â”‚         max_depth: -1                                                         
â”‚       rich_progress_bar:                                                      
â”‚         _target_: pytorch_lightning.callbacks.RichProgressBar                 
â”‚       learning_rate_monitor:                                                  
â”‚         _target_: pytorch_lightning.callbacks.LearningRateMonitor             
â”‚         logging_interval: epoch                                               
â”‚                                                                               
â”œâ”€â”€ logger
â”‚   â””â”€â”€ wandb:                                                                  
â”‚         _target_: pytorch_lightning.loggers.wandb.WandbLogger                 
â”‚         save_dir: .                                                           
â”‚         offline: false                                                        
â”‚         id: null                                                              
â”‚         anonymous: null                                                       
â”‚         project: DAG_GCN_optuna                                               
â”‚         log_model: false                                                      
â”‚         prefix: ''                                                            
â”‚         group: ''                                                             
â”‚         tags: []                                                              
â”‚         job_type: ''                                                          
â”‚                                                                               
â”œâ”€â”€ trainer
â”‚   â””â”€â”€ _target_: pytorch_lightning.Trainer                                     
â”‚       default_root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/w
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       limit_val_batches: 0                                                    
â”‚       log_every_n_steps: 14                                                   
â”‚       accelerator: gpu                                                        
â”‚       devices: 1                                                              
â”‚       deterministic: false                                                    
â”‚                                                                               
â”œâ”€â”€ paths
â”‚   â””â”€â”€ root_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       data_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚       log_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace/
â”‚       output_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspa
â”‚       work_dir: /home/sj/Projects/Causal-Discovery/Pytorch-Lightning/workspace
â”‚                                                                               
â”œâ”€â”€ extras
â”‚   â””â”€â”€ ignore_warnings: false                                                  
â”‚       enforce_tags: true                                                      
â”‚       print_config: true                                                      
â”‚                                                                               
â”œâ”€â”€ task_name
â”‚   â””â”€â”€ train                                                                   
â”œâ”€â”€ tags
â”‚   â””â”€â”€ ['sachs', 'DAG_GCN']                                                    
â”œâ”€â”€ train
â”‚   â””â”€â”€ True                                                                    
â”œâ”€â”€ test
â”‚   â””â”€â”€ False                                                                   
â”œâ”€â”€ ckpt_path
â”‚   â””â”€â”€ None                                                                    
â”œâ”€â”€ seed
â”‚   â””â”€â”€ 42                                                                      
â”œâ”€â”€ loop
â”‚   â””â”€â”€ _target_: src.loops.DAG_GCN_loop.DAG_GCN_FitLoop                        
â”‚       min_epochs: 1                                                           
â”‚       max_epochs: 1000                                                        
â”‚       k_max_iter: 10                                                          
â”‚                                                                               
â””â”€â”€ optimized_metric
    â””â”€â”€ tpr                                                                     
[[36m2022-10-24 09:47:59,920[0m][[34mpytorch_lightning.utilities.seed[0m][[32mINFO[0m] - Global seed set to 42[0m
[[36m2022-10-24 09:47:59,920[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating datamodule <src.datamodules.sachs_datamodule.SachsDataModule>[0m
[[36m2022-10-24 09:47:59,921[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating model <src.models.DAG_GCN.DAG_GCN>[0m
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder'])`.
  rank_zero_warn(
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'decoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['decoder'])`.
  rank_zero_warn(
[[36m2022-10-24 09:47:59,940[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating callbacks...[0m
[[36m2022-10-24 09:47:59,940[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.ModelCheckpoint>[0m
[[36m2022-10-24 09:47:59,942[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichModelSummary>[0m
[[36m2022-10-24 09:47:59,942[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.RichProgressBar>[0m
[[36m2022-10-24 09:47:59,943[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating callback <pytorch_lightning.callbacks.LearningRateMonitor>[0m
[[36m2022-10-24 09:47:59,943[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loggers...[0m
[[36m2022-10-24 09:47:59,943[0m][[34msrc.utils.utils[0m][[32mINFO[0m] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>[0m
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in ./wandb/run-20221024_094759-1esh7rvl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-eon-115
wandb: â­ï¸ View project at https://wandb.ai/flpie/DAG_GCN_optuna
wandb: ğŸš€ View run at https://wandb.ai/flpie/DAG_GCN_optuna/runs/1esh7rvl
[[36m2022-10-24 09:48:04,563[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating trainer <pytorch_lightning.Trainer>[0m
[[36m2022-10-24 09:48:04,568[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - GPU available: True, used: True[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - TPU available: False, using: 0 TPU cores[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - IPU available: False, using: 0 IPUs[0m
[[36m2022-10-24 09:48:04,569[0m][[34mpytorch_lightning.utilities.rank_zero[0m][[32mINFO[0m] - HPU available: False, using: 0 HPUs[0m
[[36m2022-10-24 09:48:04,569[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Instantiating loop <src.loops.DAG_GCN_loop.DAG_GCN_FitLoop>[0m
[[36m2022-10-24 09:48:04,570[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Logging hyperparameters![0m
[[36m2022-10-24 09:48:04,576[0m][[34msrc.tasks.train_task[0m][[32mINFO[0m] - Starting training![0m
  0%|          | 0/7466 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7466/7466 [00:00<00:00, 75633.64it/s]
[[36m2022-10-24 09:48:04,683[0m][[34mpytorch_lightning.accelerators.gpu[0m][[32mINFO[0m] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0][0m
â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ    â”ƒ Name                          â”ƒ Type              â”ƒ Params â”ƒ
â”¡â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0  â”‚ encoder                       â”‚ DAG_GCN_Encoder   â”‚  8.9 K â”‚
â”‚ 1  â”‚ encoder.model                 â”‚ Sequential_821734 â”‚  8.8 K â”‚
â”‚ 2  â”‚ encoder.model.module_0        â”‚ Linear            â”‚    128 â”‚
â”‚ 3  â”‚ encoder.model.module_1        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 4  â”‚ encoder.model.module_1.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 5  â”‚ encoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 6  â”‚ encoder.model.module_3        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 7  â”‚ encoder.model.module_4        â”‚ BatchNorm         â”‚    128 â”‚
â”‚ 8  â”‚ encoder.model.module_4.module â”‚ BatchNorm1d       â”‚    128 â”‚
â”‚ 9  â”‚ encoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 10 â”‚ encoder.model.module_6        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 11 â”‚ encoder.model.module_6.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 12 â”‚ encoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 13 â”‚ encoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 14 â”‚ encoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 15 â”‚ encoder.model.module_9        â”‚ DenseGCNConv      â”‚     65 â”‚
â”‚ 16 â”‚ encoder.model.module_9.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 17 â”‚ decoder                       â”‚ DAG_GCN_Decoder   â”‚  8.6 K â”‚
â”‚ 18 â”‚ decoder.model                 â”‚ Sequential_8217c1 â”‚  8.6 K â”‚
â”‚ 19 â”‚ decoder.model.module_0        â”‚ DenseGCNConv      â”‚    128 â”‚
â”‚ 20 â”‚ decoder.model.module_0.lin    â”‚ Linear            â”‚     64 â”‚
â”‚ 21 â”‚ decoder.model.module_1        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 22 â”‚ decoder.model.module_1.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 23 â”‚ decoder.model.module_2        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 24 â”‚ decoder.model.module_3        â”‚ DenseGCNConv      â”‚  4.2 K â”‚
â”‚ 25 â”‚ decoder.model.module_3.lin    â”‚ Linear            â”‚  4.1 K â”‚
â”‚ 26 â”‚ decoder.model.module_4        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 27 â”‚ decoder.model.module_4.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 28 â”‚ decoder.model.module_5        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 29 â”‚ decoder.model.module_6        â”‚ Linear            â”‚  4.2 K â”‚
â”‚ 30 â”‚ decoder.model.module_7        â”‚ BatchNorm         â”‚     22 â”‚
â”‚ 31 â”‚ decoder.model.module_7.module â”‚ BatchNorm1d       â”‚     22 â”‚
â”‚ 32 â”‚ decoder.model.module_8        â”‚ LeakyReLU         â”‚      0 â”‚
â”‚ 33 â”‚ decoder.model.module_9        â”‚ Linear            â”‚     65 â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Trainable params: 17.5 K                                                        
Non-trainable params: 0                                                         
Total params: 17.5 K                                                            
Total estimated model params size (MB): 0                                       
/home/sj/.conda/envs/pl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Exception in thread MsgRouterThr:
Traceback (most recent call last):
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
    self.run()
  File "/home/sj/.conda/envs/pl/lib/python3.10/threading.py", line 953, in run
    self._target(*self._args, **self._kwargs)
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router.py", line 69, in message_loop
    msg = self._read_message()
  File "/home/sj/.conda/envs/pl/lib/python3.10/site-packages/wandb/sdk/interface/router_queue.py", line 32, in _read_message
    msg = self._response_queue.get(timeout=1)
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/queues.py", line 117, in get
    res = self._recv_bytes()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 217, in recv_bytes
    self._check_closed()
  File "/home/sj/.conda/envs/pl/lib/python3.10/multiprocessing/connection.py", line 141, in _check_closed
    raise OSError("handle is closed")
OSError: handle is closed
